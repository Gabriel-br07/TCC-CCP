%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CAPÍTULO 1 – INTRODUÇÃO
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introdução}
\label{sec:introducao}

Em nível estadual, o Tribunal de Justiça da Bahia (TJ-BA) mantém cerca de
24\,000 execuções penais ativas no Sistema Eletrônico de Execução Unificado
(SEEU). A Lei de Execução Penal (LEP) exige revisões trimestrais, requisito
confirmado pela Súmula 533 do Superior Tribunal de Justiça (STJ); isso
representa, na prática, a análise de aproximadamente 8\,000 movimentações por
mês (cerca de 267 por dia) \cite{brasil1984lep,stj2015sumula533}. 

Ainda que a digitalização dos processos reduza o manuseio físico dos autos, o
volume de informações a consultar continua elevado. No Mutirão Processual Penal
de 2024, por exemplo, foram lançados 18\,600 atos em apenas 60 dias—número que
ultrapassa a meta estabelecida na Portaria 304/2024 do Conselho Nacional de
Justiça (CNJ)—evidenciando a sobrecarga de magistrados e servidores
\cite{tjba2024mutirao,cnj2024portaria304}. Situação semelhante observou-se no
Tribunal de Justiça do Ceará (TJ-CE): a confecção manual de 146 despachos
consumiria mais de quatro horas, ao passo que um robô executa cada um em apenas
30 s \cite{tjce2023robos}. 

Diante desse cenário, este trabalho propõe e desenvolve uma \textit{pipeline} de Geração Aumentada por Recuperação (\textit{Retrieval-Augmented Generation} -- RAG) voltada ao apoio a pesquisas jurídicas relacionadas ao SEEU. O objetivo não é substituir a atuação humana na prolação de atos judiciais, mas sim propor uma arquitetura que ofereça: (i) busca semântica sobre a LEP, a Constituição Federal e demais normas correlatas; (ii) resumos e apontamentos normativos que facilitem o entendimento das execuções penais cadastradas; e (iii) sugestões de referências jurisprudenciais e doutrinárias que possam ser utilizadas pelo usuário ao redigir suas próprias decisões.

Essa abordagem devolve tempo às equipes de execução penal, melhora a qualidade
das decisões ao tornar a pesquisa jurídica mais precisa e, sobretudo, preserva
as garantias processuais. A iniciativa segue a diretriz de modernização
judicial sustentada por parcerias entre o Conselho Nacional de Justiça (CNJ) e
organismos internacionais—como o Programa das Nações Unidas para o
Desenvolvimento (PNUD)—que buscam ampliar a transparência e o acesso à Justiça
por meio de soluções digitais e governança inovadora
\cite{undp2025pnudcnj}.


A experiência do \emph{e-Government} da Estônia mostra o impacto de
infraestrutura digital robusta: 98 \% das declarações de imposto passam a ser
enviadas on-line em poucos minutos, e o mesmo percentual de empresas é
registrado eletronicamente, aumentando a rastreabilidade e reduzindo fraudes
\cite{divald2021eformalization}. Guardadas as proporções, o SEEU enfrenta
desafio semelhante — consolidar milhares de atos processuais dispersos e
assegurar as revisões trimestrais obrigatórias. O caso estoniano indica que
automação e padronização digitais podem gerar ganhos análogos de eficiência e
transparência, reforçando o valor da \emph{pipeline} RAG proposta.

Apesar dos avanços, a consulta manual a grandes acervos documentais ainda é
demorada e sujeita a erros. Pesquisas mostram que bancos de dados vetoriais e
\emph{embeddings} reduzem a latência e aumentam a precisão de recuperação
\cite{taipalus2024vector,gao2023survey}. Técnicas RAG — que combinam
Inteligência Artificial (IA) e \emph{Large Language Models} (LLMs) — despontam
como solução para consultas em linguagem natural. Revisões recentes destacam o
potencial dessa abordagem \cite{qwak2024integrating,pujiono2024implementing}.

Este trabalho apresenta o desenvolvimento de uma \textit{pipeline} RAG concebida para consultas relacionadas ao SEEU, implementada em Python, orquestrada por LangChain, com interface \textit{chatbot} em ambiente de desenvolvimento e empacotada em Docker para facilitar reprodução. A arquitetura proposta permitiria que operadores do Direito realizem buscas intuitivas e integrem a solução a sistemas externos por meio de uma \textit{Application Programming Interface} (API) \textit{Representational State Transfer} (REST) em futuras implementações.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Seção 1.1 – Objetivos
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Objetivos}
\label{sec:objetivos}

\subsection{Objetivo Geral}
Desenvolver uma arquitetura de \emph{pipeline} RAG orientada a consultas relacionadas ao SEEU, que permita a recuperação e disponibilização de dados públicos da execução penal por meio de \textit{chatbot} e API REST em ambiente de desenvolvimento, propondo um modelo conceitual que possa contribuir para a modernização dos processos judiciais e ampliar eficiência e transparência em futuras implementações.

\subsection{Objetivos específicos}
\begin{enumerate}[label=\arabic*.]
  \item Coletar e organizar dados públicos relacionados ao SEEU, legislação penal e jurisprudência;
  \item Vetorizar esses dados em um banco vetorial (FAISS, OpenSearch
        ou soluções similares);
  \item Conectar o índice vetorial ao LLM selecionado por meio de orquestração via LangChain;
  \item Propor arquitetura de \textit{pipeline} RAG com mecanismos de recuperação e geração de respostas;
  \item Disponibilizar protótipo de chatbot integrado à \emph{pipeline} em ambiente de desenvolvimento;
  \item Implementar uma API REST documentada para acesso à funcionalidade;
  \item Preparar o ambiente de empacotamento com Docker para facilitar reprodução;
  \item Propor diretrizes para futuras integrações em ambiente de produção.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Seção 1.2 – Trabalhos Correlatos
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Trabalhos Correlatos}
\label{sec:trabalhos-correlatos}

Esta seção discute estudos que aplicam RAG a contextos jurídicos ou regulatórios, evidenciando avanços e limitações relevantes para o SEEU.

\citeonline{edwards2024hybrid} apresenta um \textit{pipeline} RAG que integra grafos de conhecimento — construídos por especialistas e por LLMs — a uma base vetorial, automatizando relatórios de acreditação da \textit{Association to Advance Collegiate Schools of Business} (AACSB). O roteamento de consultas, a decomposição em subconsultas e a síntese automática de respostas reduzem o esforço humano e aumentam a transparência; entretanto, a curadoria desses grafos ainda exige validação manual \cite{edwards2024hybrid}.  
\emph{Relação com o SEEU}: o sistema proposto neste TCC adota estratégias semelhantes de roteamento e síntese de consultas, porém aplica-as ao domínio jurídico-penal. Enquanto Edwards et al. empregam grafos de conhecimento para estruturar requisitos de acreditação, a presente solução organiza dispositivos legais, súmulas e doutrinas, permitindo recuperação contextualizada de normas da execução penal e facilitando a fundamentação de decisões judiciais.

\citeonline{pujiono2024implementing} implementam um chatbot que combina \textit{embeddings} da OpenAI, armazenamento vetorial no Pinecone e geração condicionada à recuperação, respondendo a perguntas sobre normas de agências públicas. O estudo comprova a utilidade do RAG na interpretação de regulamentos, porém não trata acervos processuais volumosos \cite{pujiono2024implementing}.  
\emph{Relação com o SEEU}: o presente trabalho incorpora técnicas de indexação escalável e métricas de cobertura para manipular grande acervo de legislação e doutrinas.

\citeonline{aquino2024extracting} descreve um fluxo RAG local para extrair informações estruturadas de documentos de licitação, utilizando \textit{embeddings} BERTimbau, Chroma como \textit{vector store} e LLMs \textit{open source}. O autor relata ganhos de precisão sobre técnicas tradicionais, mas alerta para o elevado custo computacional \cite{aquino2024extracting}.  
\emph{Relação com o SEEU}: esta pesquisa adota estratégias de compressão e particionamento que reduzem o consumo de recursos, viabilizando a execução em infraestrutura de tribunal estadual sem comprometer a qualidade das respostas.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Seção 1.3 – Solução Proposta
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Solução Proposta}
\label{sub:solucao-proposta}

A solução proposta consiste em uma arquitetura de \textit{pipeline} RAG concebida e desenvolvida em ambiente de prototipagem, organizada nos seguintes módulos conceituais e implementados:

\begin{itemize}[label=\textbullet]
  \item \textbf{Extração e pré-processamento de dados relacionados ao SEEU}: coleta automatizada de documentos oficiais (PDF) e sítios web públicos com informações de suporte;
  \item \textbf{Acesso a acervos legislativos e jurisprudenciais}: recuperação periódica de decisões do STF, STJ, TRF da 4ª Região, legislação penal, Constituição Federal, Código de Processo Penal, súmulas e resoluções em acervos oficiais;
  \item \textbf{Pré-processamento}: limpeza e segmentação textual em \textit{chunks};
  \item \textbf{Vetorização e indexação}: geração de \textit{embeddings} e armazenamento em base vetorial FAISS;
  \item \textbf{Orquestração}: gerenciamento via LangChain, com estratégia de \textit{fallback} e controle de \textit{prompt};
  \item \textbf{Interface conversacional (protótipo)}: \textit{chatbot} e API REST documentada em ambiente de desenvolvimento;
  \item \textbf{Camada de validação jurídica}: mecanismos propostos para reduzir alucinações do modelo;
  \item \textbf{Monitoramento e métricas}: estrutura para coleta de tempo de resposta e taxa de erro;
  \item \textbf{\textit{Deployment} conteinerizado}: empacotamento em Docker para facilitar reprodução e futuras implantações;
  \item \textbf{Documentação técnica}: código-fonte comentado, manuais de uso e de desenvolvedor, especificação da API;
  \item \textbf{Mitigação de riscos}: diretrizes para atualização contínua de dependências e tratamento de vulnerabilidades em futuras implementações.
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CAPÍTULO 2 – FUNDAMENTAÇÃO TEÓRICA E REVISÃO DE LITERATURA
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% ============================================================
\chapter{Fundamentação Teórica e Revisão de Literatura}
\label{chap:fundamentacao_literatura}

Apresentam-se aqui os conceitos essenciais que embasam este trabalho: bancos de dados vetoriais para armazenamento e consulta de \textit{embeddings} gerados por modelos de \textit{machine learning} \cite{qwak2024integrating}; modelos de linguagem de grande porte (\textit{Large Language Models} -- LLMs) baseados em arquitetura \textit{Transformer} \cite{lewis2020rag,gao2023survey}; o método de Geração Aumentada por Recuperação (RAG), que combina recuperação de documentos e geração de texto para reduzir alucinações \cite{edwards2024hybrid}; e, finalmente, o papel do Programa das Nações Unidas para o Desenvolvimento (PNUD) e sua parceria com o CNJ em iniciativas de transformação digital e acesso à justiça \cite{undp2025sobre,undp2025pnudcnj}.

\section{Bancos de Dados Vetoriais}
\label{sec:bancos-vetoriais}

\subsection{Definição}
Bancos de dados vetoriais são sistemas especializados em armazenar, indexar e
consultar vetores em espaço multidimensional. Esses vetores — denominados
\emph{embeddings} — são gerados por modelos de \emph{machine learning} e
capturam características semânticas de dados não estruturados, como texto,
imagens, áudio e vídeo \cite{qwak2024integrating}.

\subsection{Importância}
A busca por similaridade em embeddings é fundamental para diversas
aplicações:
\begin{itemize}
  \item \textbf{Sistemas de recomendação} – identificação de itens similares às preferências do usuário;
  \item \textbf{Busca semântica} – consultas que interpretam o significado das palavras, não apenas correspondências exatas;
  \item \textbf{Reconhecimento de padrões} – detecção de faces, objetos ou outros padrões em grandes volumes de dados;
  \item \textbf{Pipelines de IA} – armazenamento eficiente de embeddings utilizados por modelos de \emph{deep learning}.
\end{itemize}
Tais bases oferecem consultas rápidas, baixa latência e alta escalabilidade —
qualidades essenciais em Natural Language Processing (\emph{NLP}), visão computacional e outros domínios que
envolvem grandes conjuntos de dados não estruturados
\cite{qwak2024integrating}.

\subsection{Análise comparativa de soluções}
\begin{description}
  \item[Elasticsearch:] plataforma distribuída e escalável com suporte a busca vetorial pelo \texttt{Elastic Vector Search}. Limitação: implementação vetorial ainda pouco madura em alta dimensionalidade.

  \item[OpenSearch:] \emph{fork} aberto do Elasticsearch com recursos vetoriais nativos e manutenção comunitária. Limitação: requer ajustes finos para consultas complexas.

  \item[PostgreSQL + \texttt{pgvector}:] integra dados relacionais e vetoriais em um mesmo SGBD. Limitação: desempenho inferior em buscas de larga escala.

  \item[Milvus:] banco vetorial especializado, otimizado para similaridade e escalável a bilhões de vetores. Limitação: maior complexidade de configuração e manutenção.

  \item[FAISS:] biblioteca de alto desempenho amplamente utilizada em pesquisa. Limitação: não é um SGBD completo, exigindo integração adicional.

  \item[Weaviate:] código aberto que combina buscas vetoriais e de grafo, permitindo consultas semânticas e relacionais. Limitação: requer \emph{tuning} avançado para desempenho ótimo.

  \item[Oracle Vector DB:] integração nativa ao ecossistema Oracle, com alta performance e segurança empresarial. Limitação: licenciamento oneroso e menor flexibilidade frente a soluções abertas \cite{oracle2025vector}.

  \item[IBM Vector DB:] forte integração com ferramentas de IA da IBM, oferecendo recursos robustos de análise vetorial. Limitação: custo elevado e configuração complexa \cite{ibm2025vector}.
  \end{description}

  \subsection{Conclusão}
Nesta seção, foram apresentados os principais conceitos e aplicações dos bancos de dados vetoriais, bem como uma análise comparativa das soluções mais utilizadas no mercado. Verificou-se que a escolha da tecnologia adequada depende do equilíbrio entre desempenho, escalabilidade e requisitos organizacionais. Plataformas especializadas, como Milvus e FAISS, oferecem maior eficiência em buscas de similaridade, enquanto soluções integradas, como PostgreSQL+pgvector, favorecem a unificação de dados em um único Sistema Gerenciador de Banco de Dados (SGBD). O contínuo aprimoramento nas técnicas de indexação e compressão de \textit{embeddings} consolidará ainda mais o papel dos bancos vetoriais em projetos de IA e processamento de dados semiestruturados.

%--------------------------------------------------------------------
% ------------------------------------------------------------
\section{Large Language Models (LLMs)}
\label{sec:llm}

\subsection{Introdução}
Os Modelos de LLMs, baseados na arquitetura
Transformer \cite{vaswani2017attention,naveeda2024comprehensive}, elevaram o
estado da arte em Processamento de Linguagem Natural (PLN), permitindo síntese,
tradução e interpretação semântica de documentos em larga escala. Essas redes neurais podem substituir buscas
puramente lexicais por consultas semânticas, aumentando a agilidade e a
precisão das respostas. A integração de LLMs a bancos de dados vetoriais
\cite{taipalus2024vector,qwak2024integrating} reforça essa capacidade,
fornecendo resultados contextualizados a partir de extensos acervos
documentais.

\subsection{Aspectos Técnicos}
\begin{enumerate}[label=\textbf{2.\arabic*}, leftmargin=*]
  \item \textbf{Pre-Training}\label{itm:pretraining}\\
        O modelo é submetido a um corpus genérico e volumoso para capturar
        padrões linguísticos amplos, formando uma base de conhecimento
        diversificada \cite{naveeda2024comprehensive}.
  
  \item \textbf{Fine-Tuning}\label{itm:finetuning}\\
        Realiza-se \emph{fine-tuning} com dados do domínio de conhecimento desejado.
        Estratégias de regularização (e.g., \textit{dropout}, \textit{early
        stopping}) evitam \textit{overfitting}. A eficácia é medida por
        precisão, \textit{recall} e F1-score
        \cite{yue2023disclawllm,lai2023lawm}.
  
  \item \textbf{Validação e Resultados Esperados}\label{itm:validacao}\\
        Na literatura especializada, trabalhos que aplicam \textit{fine-tuning} 
        de LLMs a domínios específicos reportam avaliações baseadas em: 
        (i) precisão na recuperação de informações; (ii) qualidade das 
        respostas validadas por especialistas; e (iii) eficiência computacional 
        comparada a métodos tradicionais. De modo geral, esses estudos apontam 
        ganhos expressivos de precisão e \textit{recall} quando modelos de 
        linguagem são adaptados a corpora específicos \cite{yue2023disclawllm,lai2023lawm}. 
        Tais métricas servem de referência para futuras avaliações da arquitetura 
        proposta neste trabalho.
\end{enumerate}

\subsection{Conclusão}
A combinação de pré-treinamento e \textit{fine-tuning}, aliada a bancos vetoriais, constitui abordagem inovadora para consultas jurídicas, reduzindo prazos e aumentando a transparência do Judiciário \cite{belarmino2025aplicacao,divald2021eformalization}. A integração desses modelos a mecanismos de recuperação documental, conforme descrito na seção seguinte, potencializa a precisão e a confiabilidade das respostas geradas.

%--------------------------------------------------------------------
\section{Retrieval-Augmented Generation (RAG)}
\label{sec:rag}

\subsection{Introdução}
O RAG associa a competência de LLMs em gerar texto à recuperação automática de
documentos, reduzindo \textit{alucinações} ao fundamentar as respostas em
evidências externas verificáveis
\cite{lewis2020rag,gao2023survey,edwards2024hybrid,pujiono2024implementing}.
LLMs armazenam conhecimento nos parâmetros (\emph{memória paramétrica}); já o
RAG adiciona uma \emph{memória não paramétrica} consultável em tempo real,
essencial em cenários como o SEEU, cujo acervo documental é volumoso e
dinâmico.

\subsection{Fundamentos}
\textbf{Data retrieval.} Consultas e documentos são convertidos em
\emph{embeddings}; métodos densos, como o \textit{Dense Passage Retrieval}
(DPR), aproximam vetores por similaridade de cosseno ou distância euclidiana,
retornando um subconjunto $k$-relevante
\cite{lewis2020rag,taipalus2024vector,mageirakos2025cracking}.\\
\textbf{Content generation.} Um modelo \emph{encoder--decoder} (ex.: BART ou
T5) concatena os trechos recuperados ao \textit{prompt} e gera a resposta. O
treinamento conjunto (Sec.~\ref{sec:rag:pipeline}) ensina o \textit{retriever}
a apresentar evidências úteis ao gerador
\cite{aquino2024extracting,belarmino2025aplicacao}.

\subsection{Pipeline}
\label{sec:rag:pipeline}
\begin{enumerate}[label=\arabic*.]
  \item \textbf{Ingestion} – extração de fontes estruturadas (bases SQL,Comprehensive Knowledge Archive Network (CKAN))
        e não estruturadas (PDF, HTML); limpeza, segmentação em parágrafos e
        criação de embeddings com modelos como \textit{all-MiniLM}.
        Objetos $\langle\text{ID},\,\text{embedding},\,\text{metadata}\rangle$
        são indexados em repositórios vetoriais (FAISS, Pinecone)
        \cite{qwak2024integrating,taipalus2024vector}.
  \item \textbf{Retrieval} – a consulta é vetorizada e comparada com o índice;
        top-$k$ documentos são ranqueados. Estratégias \emph{re-rank} com
        \textit{cross-encoders} ou fusão heurística (ex.: \textit{Reciprocal
        Rank Fusion}) aumentam precisão \cite{edwards2024hybrid}.
  \item \textbf{Treinamento conjunto} – ajuste \textit{end-to-end} de
        \textit{retriever} e \textit{generator} via
        \textit{maximum-likelihood} ou \textit{policy-gradient}, fazendo o
        \textit{retriever} maximizar a probabilidade da resposta correta
        \cite{zhang2025fine}.
\end{enumerate}

\subsection{Variantes}
\begin{itemize}
  \item \textbf{RAG-Sequence} – para cada um dos $k$ documentos recuperados, o modelo gera uma resposta completa de forma independente. Em seguida, calcula-se a probabilidade de cada resposta e faz a marginalização, ou seja, a combinação ponderada dessas probabilidades para produzir a resposta final. Esse método garante que cada documento tenha igual oportunidade de influenciar a resposta global, sendo indicado quando se deseja explorar várias interpretações completas antes da decisão final \cite{lewis2020rag,edwards2024hybrid}.
  \item \textbf{RAG-Token} – em vez de gerar respostas inteiras por documento, o modelo reavalia a distribuição de probabilidade a cada novo token, permitindo que diferentes documentos contribuam de forma pontual ao longo da geração. Isso amplia a cobertura informativa e mistura evidências de várias fontes, mas requer mecanismos adicionais de coerência para evitar que o texto final fique fragmentado ou inconsistente \cite{zhang2025fine}.
\end{itemize}

\subsection{Desafios e limitações}
\begin{itemize}
  \item \textbf{Latência} – cada consulta envolve busca vetorial $+$ geração,
        podendo ultrapassar limites de tempo real
        \cite{scalable2025overload}.
  \item \textbf{Atualização em tempo real} – garantir que o índice reflita
        alterações frequentes do corpus demanda pipelines de reingestão
        contínua \cite{taipalus2024vector}.
  \item \textbf{Qualidade da recuperação} – ruído ou pouca cobertura no índice
        reduz acurácia; técnicas de \textit{negative-sampling} e \textit{hard
        negatives} no treinamento mitigam o problema
        \cite{gao2023survey,salemi2024hallucination}.
  \item \textbf{Coerência textual} – fusão de múltiplas fontes pode gerar
        redundância ou mudança de estilo; pós-edição automática e
        penalidades de repetição auxiliam \cite{zhang2025fine}.
\end{itemize}

\subsection{Conclusão}
Esta seção apresentou o método RAG, que combina modelos de linguagem de grande porte com recuperação de documentos para reduzir alucinações e fundamentar respostas em evidências verificáveis. Descreveu-se o pipeline de ingestão, recuperação e treinamento conjunto, bem como variantes como RAG-Sequence e RAG-Token. Foram discutidos desafios relativos à latência, atualização em tempo real, qualidade da recuperação e coerência textual. Conclui-se que, apesar das limitações, o RAG representa avanço significativo para aplicações que exigem precisão e atualização dinâmica, sendo promissor para sistemas que trabalham com grandes acervos documentais, como o SEEU.

% ------------------------------------------------------------
\section{Programa das Nações Unidas para o Desenvolvimento (PNUD)}
\label{sec:pnud}

O PNUD é a agência da Organização das Nações Unidas (ONU) responsável por promover o desenvolvimento
humano sustentável e erradicar a pobreza em mais de 170 países e territórios
\cite{undp2025sobre,undp2025onu}. Sediado em Nova York, o PNUD oferece suporte
técnico e financeiro a políticas públicas voltadas às populações mais
vulneráveis.

\subsection{Objetivos e mandato}
O mandato do PNUD abrange quatro eixos centrais:
\begin{itemize}
  \item \textbf{Erradicação da pobreza} – programas para reduzir a pobreza
  extrema e melhorar as condições de vida;
  \item \textbf{Desigualdade e inclusão social} – políticas que promovem
  igualdade de oportunidades;
  \item \textbf{Desenvolvimento sustentável} – iniciativas que conciliam o uso
  de recursos naturais e a proteção ambiental;
  \item \textbf{Governança democrática} – fortalecimento institucional,
  transparência e participação cidadã.
\end{itemize}
Tais ações alinham-se à Agenda~2030 e aos Objetivos de Desenvolvimento
Sustentável (ODS), sobretudo o ODS~1 (pobreza) e o ODS~10 (redução das
desigualdades) \cite{wikipedia2025pnud}.

\subsection{Estrutura e funcionamento}
Financiado por contribuições voluntárias de Estados-membros, setor privado e
ONGs, o PNUD é chefiado por um administrador indicado pelo Secretário-Geral da
ONU e aprovado pela Assembleia Geral \cite{undp2025onu}. No Brasil, opera em
parceria com governos, sociedade civil e empresas, direcionando projetos que
fomentam o desenvolvimento sustentável e reduzem desigualdades
\cite{undp2025sobre}.

\subsection{Parceria CNJ–PNUD: direitos humanos e acesso à justiça}
\label{sec:cnj-pnud}

Em 2025, o CNJ e o PNUD firmaram acordo de
cooperação para fortalecer o Poder Judiciário na promoção de direitos humanos,
sustentabilidade socioambiental e acesso à justiça por populações
vulnerabilizadas \cite{undp2025pnudcnj}. O projeto complementa iniciativas como:
\begin{itemize}
  \item \textbf{Programa Justiça 4.0} – transformação digital do Judiciário
        brasileiro, ampliando transparência e celeridade processual;
  \item \textbf{Fazendo Justiça} – melhorias nas políticas de privação de
        liberdade e reintegração social.
\end{itemize}

A juíza auxiliar Karen Luise destaca que a ação se alinha à Estratégia 2021-2026
do CNJ, priorizando igualdade e acesso jurisdicional. Para o PNUD, a parceria
reforça o ODS~16, que visa instituições eficazes e inclusivas
\cite{undp2025pnudcnj}. As atividades previstas contemplam:
\begin{enumerate}
  \item fortalecimento institucional e capacitação de magistrados;
  \item diagnósticos situacionais e desenvolvimento de metodologias inclusivas;
  \item projetos-piloto focados em crianças e adolescentes em abrigamento,
        mulheres, pessoas LGBTQIA$+$, povos indígenas, pessoas em situação de
        rua, idosos, pessoas com deficiência e grupos vulneráveis por fatores
        socioambientais ou raciais.
\end{enumerate}

\subsection*{Impacto esperado}
O fortalecimento do sistema judiciário — por meio da digitalização,
capacitação e práticas inovadoras — tende a ampliar o acesso efetivo à justiça e
a reduzir barreiras estruturais. A cooperação CNJ–PNUD, portanto, contribui para
o cumprimento dos compromissos internacionais do Brasil relacionados aos ODS,
promovendo uma sociedade mais justa e inclusiva
\cite{undp2025pnudcnj}.

\subsection{Conclusão}
O PNUD configura-se como agente estratégico na promoção do desenvolvimento humano sustentável e na erradicação da pobreza, atuando em consonância com a Agenda 2030 e os Objetivos de Desenvolvimento Sustentável. A parceria firmada em 2025 entre o CNJ e o PNUD reforça esse compromisso, ao incorporar iniciativas de transformação digital, capacitação institucional e inclusão social no âmbito do Poder Judiciário brasileiro. Espera-se que tais ações ampliem o acesso efetivo à justiça para grupos vulnerabilizados e fortaleçam a governança democrática, contribuindo para o cumprimento das metas internacionais do Brasil e para a construção de uma sociedade mais equânime e participativa.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CAPÍTULO 3 – TECNOLOGIAS
% NOTE: Mantido como Cap. 3 após reorganização (antes do Cap. 4 - Metodologia
%       ser movido para arquivo separado).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Tecnologias}

\label{chap:tecnologias}
Este capítulo apresenta as tecnologias empregadas: linguagens e bibliotecas de Machine Learning(ML)/NLP, indexação vetorial, modelos de linguagem, orquestração de pipeline e infraestrutura de deployment.  
\section{Linguagem e Bibliotecas Principais}
\begin{itemize}[label=\textbullet]

\item \textbf{Python} – linguagem base, com extensas bibliotecas para NLP e ML  \cite{python2024reference};
\item \textbf{Pandas} – manipulação de dados tabulares e pré-processamento de textos  \cite{pandas2024};
\item \textbf{NumPy} – operações vetoriais e matrizes de alto desempenho \cite{numpy2025};
\item \textbf{Scikit-learn} - para algoritmos para classificação, regressão, clustering e pré-processamento \cite{scikit-learn};



\end{itemize}

\section{Indexação e Recuperação Vetorial}
\subsection{FAISS}
FAISS (Facebook AI Similarity Search) é uma biblioteca de código aberto desenvolvida pelo Facebook AI Research (FAIR) para busca de similaridade vetorial de alta performance em larga escala. Especializada em operações eficientes com bilhões de vetores, utiliza técnicas avançadas como quantização de produto (PQ) e índices invertidos (IVF) para acelerar consultas k-NN em até 5x comparado a soluções convencionais. Sua arquitetura é otimizada para paralelismo em CPU/GPU e permite aplicações em tempo real como sistemas de recomendação, clustering de embeddings e RAG. O FAISS foi escolhido para este projeto por sua eficiência, maturidade e ampla adoção na comunidade acadêmica e industrial. Embora não gerencie metadados ou persistência nativamente, sua integração com sistemas externos como arquivos Parquet e bancos SQL permite implementar cenários completos de produção \cite{facebook2024faiss,taipalus2024vector}.


\section{Modelos de Linguagem}
\subsection{OpenAI GPT}
Para este projeto, foi selecionado o modelo de linguagem da OpenAI (GPT) como LLM principal da \textit{pipeline} RAG. Os modelos GPT (Generative Pre-trained Transformer) são baseados na arquitetura Transformer \cite{vaswani2017attention} e são treinados em vastos corpora textuais, permitindo geração de texto coerente e contextualizado em linguagem natural. A escolha da OpenAI justifica-se pela sua API estável, documentação robusta, ampla adoção em ambientes de produção e capacidade comprovada de gerar respostas de alta qualidade quando integrada a sistemas de recuperação de informações. A integração com a OpenAI API permite que a \textit{pipeline} RAG envie o contexto recuperado (documentos relevantes) junto ao \textit{prompt} do usuário, resultando em respostas fundamentadas em fontes verificadas e com menor taxa de alucinação.

\section{Ferramentas de Extração e Coleta de Dados}
\subsection{Playwright}
A coleta automatizada de dados em portais judiciais apresenta desafios técnicos significativos, sobretudo quando o conteúdo é renderizado dinamicamente por JavaScript. Nesse contexto, o Playwright, biblioteca de automação de navegadores desenvolvida pela Microsoft, foi selecionado para os módulos de \textit{scraping} deste projeto. Sua capacidade de controlar navegadores Chromium, Firefox e WebKit de forma programática permite acessar páginas web complexas, executando-as em modo \textit{headless} para extração de conteúdo textual e metadados. Além disso, a biblioteca oferece mecanismos de interação com elementos da página --- tais como cliques e preenchimento de formulários --- de forma automatizada e resiliente, o que se revelou fundamental para a coleta sistemática de jurisprudência e documentos normativos dos portais do STF, TRF4 e SEEU. Dessa forma, o Playwright constitui alicerce tecnológico para a aquisição consistente e escalável dos dados que alimentam o \textit{pipeline} RAG.

\subsection{BeautifulSoup}
Complementarmente, a análise estrutural dos documentos HTML e XML obtidos requer ferramentas especializadas em \textit{parsing}. BeautifulSoup, biblioteca Python amplamente utilizada nesse domínio, facilita a navegação, busca e modificação da árvore de elementos por meio de seletores CSS ou expressões XPath. Nos scrapers desenvolvidos, o BeautifulSoup é empregado tanto em conjunto com o Playwright --- para extração de conteúdo dinâmico --- quanto isoladamente, quando as páginas são estáticas. Sua sintaxe intuitiva e robustez na manipulação de HTML malformado tornaram-na ferramenta essencial para o pré-processamento e estruturação de dados coletados, especialmente na extração de campos específicos como títulos, datas, relatores e conteúdo integral de decisões judiciais. Por conseguinte, a combinação dessas duas ferramentas assegura a cobertura adequada de diferentes tipos de fontes documentais.

\section{Orquestração de Pipeline}
\subsection{LangChain}
A orquestração eficiente de múltiplos componentes de um sistema RAG incluindo modelos de linguagem, bases de conhecimento e mecanismos de recuperação constitui desafio arquitetural significativo. Para enfrentá-lo, este projeto adotou LangChain, biblioteca de código aberto especializada no desenvolvimento de aplicações baseadas em LLMs \cite{langchain2024}. Por meio dessa ferramenta, torna-se possível encadear chamadas a modelos de linguagem, gerenciar contextos conversacionais e integrar diferentes componentes --- tais como \textit{embeddings} e bases de conhecimento vetoriais --- de maneira coesa e reprodutível.

Além disso, a biblioteca oferece suporte a diversos provedores de LLMs, como OpenAI e Hugging Face, facilitando a comparação entre modelos e a substituição de componentes sem alterações estruturais na arquitetura. Essa modularidade revela-se especialmente útil em ambientes de pesquisa, nos quais diferentes configurações precisam ser testadas e avaliadas. Ademais, LangChain inclui utilitários para armazenamento de estado, integração com armazenamento vetorial e orquestração de múltiplos \textit{prompts}, permitindo criar fluxos de trabalho sofisticados para agentes conversacionais, assistentes virtuais e sistemas de busca semântica. Desse modo, a escolha do LangChain alinha-se aos objetivos de prototipagem rápida e escalabilidade futura da solução proposta.

\subsection{Controle de Prompt e Fallback}
Em sistemas baseados em modelos generativos, a confiabilidade das respostas depende fundamentalmente da qualidade e clareza das solicitações. Por conseguinte, foi implementado um mecanismo de controle de \textit{prompt} que valida a completude da requisição antes de enviá-la ao LLM. Caso o modelo retorne uma resposta vaga ou incerta, a lógica de \textit{fallback} direciona a consulta a fontes de dados oficiais ou bases de conhecimento verificadas, combinando a flexibilidade dos modelos generativos com a precisão de dados estruturados.

Ademais, o controle de \textit{prompt} permite reformular automaticamente perguntas problemáticas, melhorando a qualidade das interações e reduzindo a taxa de respostas inadequadas. O mecanismo de \textit{fallback}, por sua vez, pode acessar APIs governamentais, documentos normativos ou bases de conhecimento internas sempre que a recuperação vetorial não fornecer conteúdo suficiente. Essa estratégia híbrida assegura consistência e confiabilidade, minimizando riscos de informações errôneas aspecto crucial em aplicações jurídicas, nas quais a acurácia das informações é imperativa. Portanto, a arquitetura proposta alia a capacidade generativa dos LLMs ao respaldo factual de fontes oficiais, resultando em experiência de usuário mais robusta e confiável.

\section{Tecnologias de Interface e Frontend}
\subsection{Nuxt.js}
A construção de interfaces web modernas e interativas requer \textit{frameworks} que equilibrem produtividade de desenvolvimento, desempenho em tempo de execução e manutenibilidade de código. Nesse contexto, Nuxt.js foi selecionado como base para a interface de usuário do protótipo de \textit{chatbot} desenvolvido. Conforme a documentação oficial \cite{nuxt2024docs}, o Nuxt é um \textit{framework} de código aberto construído sobre Vue.js, orientado para criação de aplicações web \textit{full-stack} com suporte a renderização do lado do servidor (SSR), geração de sites estáticos (SSG) e modo SPA (\textit{Single Page Application}). Além disso, oferece roteamento automático, gerenciamento de estado integrado e facilidade de integração com APIs REST. Sua arquitetura modular e suporte nativo a TypeScript permitem desenvolvimento ágil e escalável de componentes de apresentação, características que se mostraram adequadas aos requisitos de prototipagem rápida do projeto. A escolha do Nuxt justifica-se, portanto, pela combinação de recursos avançados, maturidade do ecossistema e facilidade de integração com os demais componentes da arquitetura proposta.

\subsection{Vue.js}
No cerne do Nuxt.js encontra-se Vue.js, \textit{framework} JavaScript progressivo para construção de interfaces de usuário baseadas em componentes reativos \cite{vue2024docs}. Segundo a documentação oficial, o Vue adota um modelo declarativo baseado em componentes que permite construir interfaces complexas a partir de unidades reutilizáveis e auto-contidas. Essa arquitetura componentizada facilita a criação de interfaces dinâmicas e interativas, nas quais cada elemento da interface tal como o campo de entrada de consultas, o painel de exibição de respostas, a listagem de fontes citadas e os controles auxiliares é encapsulado em um componente independente e reutilizável. A reatividade nativa do Vue.js, baseada em um sistema de reatividade refinado, garante que qualquer alteração no estado da aplicação seja refletida instantaneamente na interface, proporcionando experiência de usuário fluida e responsiva. Tal característica mostra-se especialmente relevante em sistemas conversacionais, nos quais a percepção de imediatez e interatividade influencia diretamente a aceitação pelos usuários. Por conseguinte, a combinação de Vue.js como base e Nuxt.js como camada de abstração proporciona fundamento sólido para o desenvolvimento da interface do protótipo.

\section{Infraestrutura e Deployment}
\subsection{Docker}
A reprodução consistente de ambientes computacionais constitui requisito fundamental para pesquisas em Ciência da Computação e Engenharia de Software. Nesse sentido, Docker plataforma de conteinerização amplamente adotada foi selecionado para empacotar e distribuir os componentes da \textit{pipeline} RAG desenvolvida \cite{docker2024}. Cada contêiner encapsula a aplicação juntamente com suas bibliotecas, dependências e configurações, garantindo consistência entre diferentes ambientes de desenvolvimento, teste e eventual produção.

Além da reprodução, o Docker facilita o versionamento por meio de imagens imutáveis, permitindo rastrear alterações e reverter para versões anteriores quando necessário. A ferramenta também simplifica a escalabilidade de serviços, integrando-se a orquestradores como Docker Compose ou Kubernetes para gerenciar múltiplos contêineres. Ademais, a existência de um repositório público de imagens (Docker Hub) acelera o desenvolvimento ao disponibilizar soluções prontas. Dessa forma, a conteinerização com Docker não apenas atende aos requisitos de reprodução científica, mas também prepara o projeto para futuras implantações em ambientes de produção.

\subsection{Kubernetes}
Embora a conteinerização resolva questões de reprodução, a orquestração de múltiplos contêineres em escala requer plataformas especializadas. Kubernetes, sistema de código aberto amplamente utilizado para orquestração de contêineres, foi considerado na arquitetura proposta como caminho de evolução futura do projeto \cite{kubernetes2025overview}. Esse sistema permite implantar, escalar e operar aplicações em \textit{clusters} de servidores, agrupando contêineres em unidades denominadas \textit{pods} e facilitando o gerenciamento de cargas de trabalho.

Os componentes principais do Kubernetes incluindo \textit{kubelet}, \textit{API Server}, \textit{Scheduler} e \textit{Controller Manager} zelam pelo estado desejado do \textit{cluster}, permitindo escalar serviços automaticamente com base em métricas de uso e garantir alta disponibilidade. Por meio da abstração de serviços e \textit{deployment controllers}, torna-se possível realizar atualizações contínuas e \textit{rolling updates} sem interrupção de serviço. Além disso, a plataforma oferece mecanismos de descoberta de serviço, balanceamento de carga e armazenamento persistente, características essenciais para aplicações em produção. Assim, embora o protótipo atual utilize Docker de forma isolada, a arquitetura foi concebida de modo a facilitar migração futura para ambientes orquestrados por Kubernetes.

\subsection{FastAPI como Framework da API REST}
FastAPI é um \textit{framework} web moderno e de alta performance para construção de APIs em Python, baseado em anotações de tipo (\textit{type hints}) e no padrão ASGI \cite{fastapi2024docs}. Conforme sua documentação oficial, o FastAPI foi projetado para ser rápido tanto em desempenho de execução quanto em velocidade de desenvolvimento  oferecendo validação automática baseada em esquemas Python, geração automática de documentação interativa e suporte nativo a operações assíncronas. Neste projeto, o FastAPI foi escolhido como \textit{framework} principal para implementação da API REST que expõe a \textit{pipeline} RAG. Suas principais vantagens incluem: (i) validação automática de dados de entrada e saída com base em \textit{schemas} Pydantic, reduzindo erros e aumentando a robustez do código; (ii) documentação interativa automática (Swagger/OpenAPI e ReDoc), facilitando testes e integração; (iii) suporte nativo a operações assíncronas, permitindo maior concorrência e eficiência no tratamento de requisições; e (iv) desempenho comparável a \textit{frameworks} como Node.js e Go, aspecto relevante para aplicações que demandam baixa latência. O FastAPI foi utilizado para implementar os \textit{endpoints} de consulta, recuperação de metadados de \textit{clusters}, geração de respostas e integração com o modelo de linguagem da OpenAI, garantindo escalabilidade e manutenção simplificada do código.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NOTE: Os capítulos 3 (Metodologia) e 4 (Desenvolvimento) foram
%       movidos para arquivos separados: metodologia.tex e desenvolvimento.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NOTE: Os cap�tulos 3 (Metodologia) e 4 (Desenvolvimento) foram
%       movidos para arquivos separados: metodologia.tex e desenvolvimento.tex
%       Estes arquivos s�o inclu�dos no main.tex na ordem correta.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Resultados Esperados}
\label{chap:resultados}

Este capítulo descreve, de forma detalhada, os resultados que se almejou alcançar com a execução do presente Trabalho de Conclusão de Curso (TCC). Tais resultados derivam dos objetivos estabelecidos no Capítulo~\ref{sec:objetivos} e da metodologia apresentada no Capítulo~\ref{chap:metodologia}, respeitando as normas da Associação Brasileira de Normas Técnicas -- ABNT (NBR 14724:2011) para trabalhos acadêmicos.

\section{Proposta de arquitetura de \textit{pipeline} RAG funcional}
Em alinhamento ao objetivo geral e aos objetivos específicos 1 a 4 (coleta, organização, vetorização e conexão ao LLM), o trabalho propôs e desenvolveu, em ambiente de prototipação, uma arquitetura de \textit{pipeline} RAG capaz de integrar um modelo de LLM a um índice vetorial contendo documentos públicos relacionados ao SEEU, legislação penal, súmulas e doutrinas correlatas. O protótipo desenvolvido oferece interface conversacional (\textit{chatbot}) em língua portuguesa e API REST documentada em ambiente de desenvolvimento, permitindo consultas em linguagem natural com respostas fundamentadas nas fontes originais. Espera-se que essa arquitetura proposta sirva de base conceitual e técnica para futuras implementações em ambiente de produção, correspondendo aos objetivos específicos 5, 6 e 7 (disponibilização do protótipo de \textit{chatbot}, implementação da API REST documentada e preparação do ambiente de empacotamento).

\section{Potencial ganho de eficiência na recuperação de informações}
A arquitetura proposta visa proporcionar redução no tempo médio despendido pelos operadores do Direito para localizar e consolidar informações relacionadas ao SEEU, quando comparado ao processo manual atualmente utilizado. Embora testes formais de eficiência e usabilidade com usuários finais não tenham sido realizados no escopo deste trabalho, a fundamentação teórica e a arquitetura desenvolvida sugerem que futuras implementações, acompanhadas de ensaios controlados, poderão mensurar ganhos quantitativos de tempo de resposta, empregando métricas de precisão, \textit{recall} e F\textsubscript{1}. Tais avaliações constituem etapa essencial para validação em ambiente de produção e representam oportunidade para trabalhos futuros.

\section{Escalabilidade e portabilidade comprovadas}
Em conformidade com o objetivo específico 8 (preparar o ambiente de \textit{deployment} com segurança e escalabilidade), a solução será entregue em contêineres Docker orquestrados via Kubernetes, garantindo portabilidade entre ambientes e escalabilidade horizontal necessária para suportar picos de demanda. A arquitetura deverá manter latência média de recuperação inferior a 1 segundo para consultas padrão.

\section{Alinhamento institucional e potencial impacto social}
Almeja-se que a arquitetura proposta e o protótipo desenvolvido se alinhem às iniciativas de transformação digital do Conselho Nacional de Justiça (Programa Justiça 4.0) e aos Objetivos de Desenvolvimento Sustentável nº 16 da Organização das Nações Unidas, oferecendo base conceitual e técnica para futuras soluções que contribuam para a transparência e o acesso à justiça de populações vulnerabilizadas. A efetivação desse potencial dependerá de implementações subsequentes em ambiente de produção, acompanhadas de avaliações de impacto e adequação às políticas institucionais do Poder Judiciário.

\section{Base para melhoria contínua}
Será implementado um mecanismo de \emph{feedback loop} que registre as interações dos usuários e permita o re-treinamento periódico do modelo de linguagem, assegurando a evolução constante do sistema e a adaptação às mudanças normativas ou procedimentais.

\section{Documentação técnica completa}
Serão entregues: código-fonte comentado, arquivos \texttt{Dockerfile}, manual do desenvolvedor, manual do usuário final e documentação da API. Essa documentação facilitará a reprodutibilidade acadêmica e a eventual adoção da solução por outros órgãos do Judiciário.

\section{Mitigação de riscos operacionais}
O projeto contemplará um plano de mitigação de riscos que inclua atualização contínua de dependências \emph{open source}, testes automatizados de regressão e políticas de segurança da informação, garantindo a confiabilidade e a sustentabilidade da aplicação em produção.

A consecução dos resultados elencados neste capítulo demonstrará a viabilidade técnica e o impacto prático da aplicação de técnicas de RAG na execução penal brasileira, servindo de base para futuras pesquisas e para possíveis expansões em âmbito nacional.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CAPÍTULO 6 – MELHORIAS FUTURAS E JUSTIFICATIVA
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Melhorias Futuras e Justificativa}
\label{chap:melhorias}

\section{Introdução}

Este capítulo apresenta propostas de melhorias e extensões para a \textit{pipeline} RAG desenvolvida neste trabalho. As sugestões são organizadas por área temática (infraestrutura, modelos e \textit{embeddings}, indexação, módulos de extração, API/interface, testes, observabilidade e segurança) e têm por objetivo orientar trabalhos futuros, experimentos acadêmicos e evoluções para ambientes de produção.

\section{Visão Geral das Melhorias Propostas}

As principais direções de melhoria identificadas incluem:

\begin{itemize}
  \item \textbf{Escalabilidade e disponibilidade}: migração da arquitetura para suporte a índices distribuídos e serviços de alta disponibilidade em produção;
  \item \textbf{Qualidade dos \textit{embeddings}}: avaliação de modelos mais recentes e estratégias de \textit{fine-tuning} específicas para o domínio jurídico;
  \item \textbf{Robustez dos módulos de extração}: implementação de mecanismos de recuperação automática, tolerância a bloqueios e paralelização;
  \item \textbf{Observabilidade e testes}: ampliação da cobertura de testes automatizados e implementação de métricas de monitoramento em tempo real;
  \item \textbf{Segurança e governança de dados}: fortalecimento de privacidade, controle de acesso e conformidade com a Lei Geral de Proteção de Dados (LGPD).
\end{itemize}

\section{Arquitetura e Infraestrutura}

\textbf{Proposta de melhoria:} Evoluir a arquitetura para suportar execução em produção com orquestração completa via Kubernetes, armazenamento gerenciado de índices (OpenSearch/Milvus em \textit{cluster}) e \textit{pipelines} de processamento baseadas em filas (RabbitMQ, Redis ou Cloud Tasks).

\textbf{Justificativa:} A solução atual, baseada em FAISS local e \textit{scripts} ad hoc, é adequada para prototipagem, mas não oferece alta disponibilidade, replicação automática ou escalabilidade horizontal -- requisitos essenciais para uso em produção e experimentos em larga escala.

\textbf{Benefícios esperados:} Tolerância a falhas, balanceamento de carga, reinício automático de serviços, capacidade de lidar com crescimento de dados e possibilidade de realizar testes A/B entre diferentes \textit{backends} de armazenamento vetorial.

\textbf{Sugestão de implementação:} Definir infraestrutura mínima de produção utilizando Helm \textit{charts}, \textit{manifests} Kubernetes com \textit{StatefulSets} para armazenamento vetorial persistente e volumes compartilhados. Prioridade: alta.

\section{Modelos de \textit{Embeddings} e Gerenciamento de Modelos}

\textbf{Proposta de melhoria:} Avaliar modelos de \textit{embeddings} mais recentes, especialmente aqueles multilingues ou ajustados ao domínio jurídico, implementar versionamento de \textit{embeddings} e mecanismos de re-indexação incremental. Considerar técnicas de quantização e aproximação para reduzir custos de armazenamento.

\textbf{Justificativa:} A qualidade da recuperação semântica depende fundamentalmente da qualidade dos \textit{embeddings}. Modelos aprimorados ou submetidos a \textit{fine-tuning} específico para linguagem jurídica tendem a aumentar significativamente as métricas de precisão e \textit{recall}.

\textbf{Benefícios esperados:} Maior relevância nas respostas, redução de falsos positivos e melhoria nas métricas de avaliação (precisão@k, \textit{recall}@k). O versionamento permite experimentação controlada e comparação sistemática entre modelos.

\textbf{Sugestão de implementação:} Adicionar módulo de experimentação que registre modelo, hiperparâmetros e \textit{seeds} aleatórias; utilizar \textit{pipelines} automatizadas para re-indexação incremental com registro de metadados por versão. Prioridade: alta.

\section{Indexação e Backends de Busca Vetorial}

\textbf{Proposta de melhoria:} Comparar FAISS com alternativas (OpenSearch vector search, Milvus, Pinecone) considerando custo, latência e facilidade operacional; implementar suporte a \textit{sharding} e replicação; oferecer mecanismo de \textit{fallback} híbrido combinando busca vetorial e léxica (BM25).

\textbf{Justificativa:} Diferentes \textit{backends} apresentam compromissos distintos. FAISS é rápido localmente, mas não oferece clusterização nativa; OpenSearch permite consultas híbridas e persistência integrada.

\textbf{Benefícios esperados:} Menor latência sob carga, maior resiliência e melhores resultados ao combinar sinais léxicos e semânticos.

\textbf{Sugestão de implementação:} Criar \textit{benchmarks} automatizados e uma camada de abstração para alternar \textit{backends} via configuração. Prioridade: média-alta.

\section{Pipelines de Ingestão e Tratamento de Dados}

\textbf{Proposta de melhoria:} Tornar as \textit{pipelines} idempotentes, tolerantes a falhas e escaláveis; adicionar etapas de normalização jurídica, incluindo extração robusta de metadados, enriquecimento semântico e deduplicação avançada.

\textbf{Justificativa:} A qualidade dos dados de entrada impacta diretamente a qualidade da recuperação e da avaliação do sistema. \textit{Pipelines} manuais dificultam a reprodutibilidade e a auditoria do processo.

\textbf{Benefícios esperados:} Redução de ruído no índice, indexação mais rápida e reprocessamento controlado quando \textit{embeddings} ou modelos são atualizados.

\textbf{Sugestão de implementação:} Adotar \textit{frameworks} de orquestração (Airflow, Prefect) ou \textit{jobs} em Kubernetes, com armazenamento intermediário (Parquet/NDJSON) e \textit{checksums} para identificação de mudanças. Prioridade: média.

\section{Módulos de Extração Automatizada (TRF4, STF, STJ)}

\textbf{Proposta de melhoria:} Tornar os módulos de extração mais resilientes, implementando mecanismos de \textit{retry/backoff}, tratamento de CAPTCHAs e rotação de proxies; instrumentar métricas de sucesso e erro; padronizar saída (esquemas e contratos JSONL). Modularizar os componentes para reuso e permitir execução distribuída.

\textbf{Justificativa:} A extração confiável garante cobertura adequada de dados e respeito às políticas dos portais. A arquitetura atual pode ser frágil frente a mudanças de estrutura (DOM) e bloqueios.

\textbf{Benefícios esperados:} Redução de falhas, menor necessidade de intervenção manual e melhor reprodutibilidade do conjunto de dados.

\textbf{Sugestão de implementação:} Encapsular interações Playwright em adaptadores testáveis, usar filas para distribuir trabalho e registrar estados em banco de dados leve (SQLite/Redis). Prioridade: alta.

\section{API, Interface e Experiência do Usuário}

\textbf{Proposta de melhoria:} Adicionar mecanismos de autenticação e autorização (API \textit{keys}/JWT), limitação de taxa de requisições (\textit{rate limiting}), paginação e filtros avançados; implementar na interface funcionalidades de \textit{feedback} do usuário sobre relevância das respostas para coleta de sinais e treinamento posterior.

\textbf{Justificativa:} Para ambientes de produção, o controle de acesso é obrigatório. Sinais coletados dos usuários permitem medir e melhorar continuamente a relevância do sistema.

\textbf{Benefícios esperados:} Segurança do serviço, métricas de uso e ciclo de \textit{feedback} humano no processo (\textit{human-in-the-loop}) para otimização contínua.

\textbf{Sugestão de implementação:} Integrar FastAPI com \textit{middleware} de autenticação; adicionar componente de avaliação de resultado na interface, conectado a \textit{endpoint} que registra \textit{feedback}. Prioridade: média.

\section{Testes, Validação e Reprodutibilidade}

\textbf{Proposta de melhoria:} Ampliar cobertura de testes (unitários, integração, \textit{end-to-end}), adicionar testes de carga e cenários de regressão; padronizar \textit{seeds} aleatórias, salvar pontos de referência e criar \textit{scripts} de reprodutibilidade para experimentos.

\textbf{Justificativa:} A confiança nas mudanças e a capacidade de comparar configurações experimentais dependem de testes abrangentes e reprodutibilidade dos resultados.

\textbf{Benefícios esperados:} Ciclos de desenvolvimento mais rápidos, redução de regressões e resultados experimentais confiáveis para validação científica.

\textbf{Sugestão de implementação:} Ampliar conjunto de testes com \textit{fixtures} que simulem armazenamentos e modelos; integrar integração contínua (CI) via GitHub Actions para execução automática de testes e \textit{benchmarks}. Prioridade: alta.

\section{Observabilidade e Operações}

\textbf{Proposta de melhoria:} Instrumentar serviços com métricas (Prometheus), rastreamento distribuído (OpenTelemetry) e \textit{logs} estruturados; criar painéis de monitoramento (Grafana) com latência, vazão (\textit{throughput}), erros e qualidade de resposta.

\textbf{Justificativa:} O monitoramento é fundamental para identificar gargalos e regressões, além de apoiar decisões de otimização e escalonamento.

\textbf{Benefícios esperados:} Visibilidade operacional, diagnósticos mais rápidos e fundamentação para decisões de escalonamento e ajuste fino.

\textbf{Sugestão de implementação:} Adicionar \textit{middleware} para métricas no FastAPI, exportadores para Prometheus e coletores de rastreamento. Prioridade: média.

\section{Segurança, Privacidade e Governança de Dados}

\textbf{Proposta de melhoria:} Implementar anonimização de dados sensíveis, políticas de retenção, \textit{logs} auditáveis e controle de acesso granular; avaliar conformidade com normas aplicáveis, especialmente a Lei Geral de Proteção de Dados (LGPD).

\textbf{Justificativa:} O projeto lida com documentos jurídicos que podem conter dados pessoais. A conformidade legal é imprescindível para uso além do contexto de pesquisa acadêmica.

\textbf{Benefícios esperados:} Mitigação de riscos legais, maior aceitação institucional e proteção adequada de dados pessoais.

\textbf{Sugestão de implementação:} Criar módulo de sanitização de textos antes da indexação, implementar consentimento informado e processar dados pessoais com marcadores e técnicas de ocultação (\textit{redaction}). Prioridade: alta.

\section{Documentação, Reprodutibilidade e Processo de Pesquisa}

\textbf{Proposta de melhoria:} Centralizar documentação operacional, apresentar instruções reproduzíveis para experimentos (datasets, \textit{seeds}, versões de modelos) e exemplos “how‑to” para ingressar novos colaboradores.

\textbf{Justificativa:} A documentação abrangente facilita a avaliação no contexto acadêmico e acelera a adoção por outros pesquisadores.

\textbf{Benefícios esperados:} Redução da barreira de entrada, facilidade para replicação e validação científica dos resultados.

\textbf{Sugestão de implementação:} Atualizar documentação principal do projeto, criar manuais operacionais e adicionar seção de experimentos com \textit{scripts} automatizados. Prioridade: média.

\section{Cronograma de Implementação Sugerido}

Com base nas prioridades identificadas, propõe-se o seguinte cronograma de implementação das melhorias:

\begin{enumerate}
  \item \textbf{Curto prazo (1 a 3 meses):} Fortalecimento dos módulos de extração, versionamento de \textit{embeddings}, implementação de autenticação na API e ampliação de testes unitários.
  \item \textbf{Médio prazo (3 a 9 meses):} Orquestração de \textit{pipelines}, avaliação comparativa de \textit{backends} vetoriais, implementação de monitoramento básico com Prometheus e Grafana.
  \item \textbf{Longo prazo (9 a 18 meses):} Migração para infraestrutura clusterizada com Kubernetes, suporte a reindexação em larga escala, \textit{fine-tuning} de modelos jurídicos e publicação de conjunto de dados documentado para reprodutibilidade.
\end{enumerate}

\section{Considerações Finais do Capítulo}

As melhorias propostas neste capítulo visam evoluir a \textit{pipeline} RAG implementada em uma plataforma robusta para experimentação acadêmica e, potencialmente, uso em ambientes de produção. As prioridades destacadas equilibram ganhos de qualidade (aprimoramento de \textit{embeddings}, \textit{pipelines} e módulos de extração) com necessidades operacionais (segurança, observabilidade e infraestrutura escalável). Implementações incrementais, acompanhadas de testes sistemáticos e documentação adequada, permitirão avaliar o impacto de cada melhoria e ajustar a trajetória conforme os resultados experimentais obtidos.

\section{Recomendações para Trabalhos Futuros}

Com base na análise apresentada, recomendam-se as seguintes ações para trabalhos futuros:

\begin{itemize}
  \item Selecionar 2 a 3 itens prioritários (por exemplo: robustez dos módulos de extração, versionamento de \textit{embeddings} e ampliação de testes com integração contínua) e estabelecer tarefas com critérios de aceitação bem definidos.
  \item Implementar avaliação comparativa (\textit{benchmark}) entre FAISS e OpenSearch considerando latência e precisão no conjunto de dados atual.
  \item Planejar revisão de segurança e privacidade com orientador e, se necessário, consultor especializado em proteção de dados.
\end{itemize}

\begin{itemize}
  \item Selecionar 2 a 3 itens prioritários (por exemplo: robustez dos módulos de extração, versionamento de \textit{embeddings} e ampliação de testes com integração contínua) e estabelecer tarefas com critérios de aceitação bem definidos.
  \item Implementar avaliação comparativa (\textit{benchmark}) entre FAISS e OpenSearch considerando latência e precisão no conjunto de dados atual.
  \item Planejar revisão de segurança e privacidade com orientador e, se necessário, consultor especializado em proteção de dados.
\end{itemize}
