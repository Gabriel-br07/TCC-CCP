%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CAPÍTULO 1 – INTRODUÇÃO
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introdução}
\label{sec:introducao}

Em nível estadual, o Tribunal de Justiça da Bahia (TJ-BA) mantém cerca de
24\,000 execuções penais ativas no Sistema Eletrônico de Execução Unificado
(SEEU). A Lei de Execução Penal (LEP) exige revisões trimestrais, requisito
confirmado pela Súmula 533 do Superior Tribunal de Justiça (STJ); isso
representa, na prática, a análise de aproximadamente 8\,000 movimentações por
mês (cerca de 267 por dia) \cite{brasil1984lep,stj2015sumula533}. 

Ainda que a digitalização dos processos reduza o manuseio físico dos autos, o
volume de informações a consultar continua elevado. No Mutirão Processual Penal
de 2024, por exemplo, foram lançados 18\,600 atos em apenas 60 dias—número que
ultrapassa a meta estabelecida na Portaria 304/2024 do Conselho Nacional de
Justiça (CNJ)—evidenciando a sobrecarga de magistrados e servidores
\cite{tjba2024mutirao,cnj2024portaria304}. Situação semelhante observou-se no
Tribunal de Justiça do Ceará (TJ-CE): a confecção manual de 146 despachos
consumiria mais de quatro horas, ao passo que um robô executa cada um em apenas
30 s \cite{tjce2023robos}. 

Diante desse cenário, este trabalho propõe uma \emph{pipeline} de
\emph{Retrieval-Augmented Generation} (RAG) voltada **exclusivamente** ao apoio
pesquisas jurídicas no contexto do SEEU. O objetivo não é substituir a atuação
humana na prolação de atos judiciais, tampouco “processar processos penais”, mas
sim oferecer busca semântica sobre a LEP, a Constituição Federal e demais normas
correlatas; resumos e apontamentos normativos que facilitem o entendimento das execuções penais cadastradas; sugestões de referências
jurisprudenciais e doutrinárias que possam ser utilizadas pelo usuário ao redigir suas próprias minutas.

Essa abordagem devolve tempo às equipes de execução penal, melhora a qualidade
das decisões ao tornar a pesquisa jurídica mais precisa e, sobretudo, preserva
as garantias processuais. A iniciativa segue a diretriz de modernização
judicial sustentada por parcerias entre o Conselho Nacional de Justiça (CNJ) e
organismos internacionais—como o Programa das Nações Unidas para o
Desenvolvimento (PNUD)—que buscam ampliar a transparência e o acesso à Justiça
por meio de soluções digitais e governança inovadora
\cite{undp2025pnudcnj}.


A experiência do \emph{e-Government} da Estônia mostra o impacto de
infraestrutura digital robusta: 98 \% das declarações de imposto passam a ser
enviadas on-line em poucos minutos, e o mesmo percentual de empresas é
registrado eletronicamente, aumentando a rastreabilidade e reduzindo fraudes
\cite{divald2021eformalization}. Guardadas as proporções, o SEEU enfrenta
desafio semelhante — consolidar milhares de atos processuais dispersos e
assegurar as revisões trimestrais obrigatórias. O caso estoniano indica que
automação e padronização digitais podem gerar ganhos análogos de eficiência e
transparência, reforçando o valor da \emph{pipeline} RAG proposta.

Apesar dos avanços, a consulta manual a grandes acervos documentais ainda é
demorada e sujeita a erros. Pesquisas mostram que bancos de dados vetoriais e
\emph{embeddings} reduzem a latência e aumentam a precisão de recuperação
\cite{taipalus2024vector,gao2023survey}. Técnicas RAG — que combinam
Inteligência Artificial (IA) e \emph{Large Language Models} (LLMs) — despontam
como solução para consultas em linguagem natural. Revisões recentes destacam o
potencial dessa abordagem \cite{qwak2024integrating,pujiono2024implementing}.

Este trabalho propõe construir uma \emph{pipeline} RAG para o SEEU,
implementada em Python, orquestrada por LangChain, exposta por chatbot em Flask
e empacotada em Docker para implantação consistente. Operadores do Direito
poderão realizar buscas intuitivas e integrar a solução a sistemas externos por
meio de uma Application Programming Interface (API) Representational State Transfer (REST).




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Seção 1.1 – Objetivos
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Objetivos}
\label{sec:objetivos}

\subsection{Objetivo Geral}
Desenvolver, para o SEEU, uma \emph{pipeline} RAG que automatize a recuperação
e a disponibilização de dados públicos da execução penal, oferecendo chatbot e
API REST, a fim de modernizar os processos judiciais e ampliar eficiência e
transparência.

\subsection{Objetivos específicos}
\begin{enumerate}[label=\arabic*.]
  \item Coletar e organizar os dados públicos do SEEU;
  \item Vetorizar esses dados em um banco vetorial (Elasticsearch, OpenSearch
        ou PostgreSQL + pgvector);
  \item Conectar o índice vetorial ao LLM selecionado;
  \item Testar a eficiência e a qualidade das respostas;
  \item Disponibilizar um chatbot integrado à \emph{pipeline};
  \item Conduzir testes de usabilidade e ajustar a experiência do usuário;
  \item Implementar uma API REST documentada;
  \item Preparar o ambiente de \emph{deployment} com segurança e escalabilidade.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Seção 1.2 – Trabalhos Correlatos
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Trabalhos Correlatos}
\label{sec:trabalhos-correlatos}

Esta seção discute estudos que aplicam RAG a contextos jurídicos ou regulatórios, evidenciando avanços e limitações relevantes para o SEEU.

\citeonline{edwards2024hybrid} apresenta um \textit{pipeline} RAG que integra grafos de conhecimento — construídos por especialistas e por LLMs — a uma base vetorial, automatizando relatórios de acreditação Association to Advance Collegiate Schools of Business (AACSB). O roteamento de consultas, a decomposição em subconsultas e a síntese automática de respostas reduzem o esforço humano e aumentam a transparência; entretanto, a curadoria desses grafos ainda exige validação manual \cite{edwards2024hybrid}.  
\emph{Relação com o SEEU}: TODO: explicar melhor essa relação. o sistema proposto neste TCC adota roteamento e síntese similares, mas aplica regras jurídicas automáticas para validar o grafo, minimizando a intervenção humana.

\citeonline{pujiono2024implementing} implementam um chatbot que combina \textit{embeddings} da OpenAI, armazenamento vetorial no Pinecone e geração condicionada à recuperação, respondendo a perguntas sobre normas de agências públicas. O estudo comprova a utilidade do RAG na interpretação de regulamentos, porém não trata acervos processuais volumosos \cite{pujiono2024implementing}.  
\emph{Relação com o SEEU}: o presente trabalho incorpora técnicas de indexação escalável e métricas de cobertura para manipular grande acervo de legislação e doutrinas.

\citeonline{aquino2024extracting} descreve um fluxo RAG local para extrair informações estruturadas de documentos de licitação, utilizando \textit{embeddings} BERTimbau, Chroma como \textit{vector store} e LLMs \textit{open source}. O autor relata ganhos de precisão sobre técnicas tradicionais, mas alerta para o elevado custo computacional \cite{aquino2024extracting}.  
\emph{Relação com o SEEU}: esta pesquisa adota estratégias de compressão e particionamento que reduzem o consumo de recursos, viabilizando a execução em infraestrutura de tribunal estadual sem comprometer a qualidade das respostas.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Seção 1.3 – Solução Proposta
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Solução Proposta}
\label{sub:solucao-proposta}

A solução consiste em uma \textit{pipeline} RAG organizada nos seguintes módulos:

\begin{itemize}[label=\textbullet]
  \item \textbf{Extração e pré-processamento de dados relacionado ao SEEU} de documentos oficiais (PDF), sítios web públicos com informações de suporte, conversão de vídeos de treinamento para aúdio e posteriormente para texto;
  \item \textbf{Acesso à acervos legislativos}: fazer a recuperação bimestral da legislação penal, constituição federal, código de processo penal, súmulas e resoluções em acervos oficiais do Estado;
  \item \textbf{Pré-processamento} — limpeza,  e segmentação em \emph{chunks};
  \item \textbf{Vetorização} e indexação em base vetorial (Facebook AI Similarity Search (FAISS) ou OpenSearch) - embeddings;
  \item \textbf{Orquestração} via LangChain, com fallback híbrido (keyword + semantic);
  \item \textbf{Interface conversacional} (chatbot) e \textbf{API REST} documentada em OpenAPI;
  \item \textbf{Camada de validação jurídica} para reduzir alucinações;
  \item \textbf{Testes automatizados} de integração e regressão (precisão, recall, F1);
  \item \textbf{Monitoramento e métricas} (tempo de resposta, taxa de erro, taxa de alucinação);
  \item \textbf{Deployment conteinerizado} (Docker/Kubernetes) com Continuous Integration / Continuous Deployment (CI/CD);
  \item \textbf{Documentação técnica completa} (código-fonte comentado, manuais, API);
  \item \textbf{Plano de mitigação de riscos} e atualização contínua de dependências.
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CAPÍTULO 2 – FUNDAMENTAÇÃO TEÓRICA E REVISÃO DE LITERATURA
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% ============================================================
\chapter{Fundamentação Teórica e Revisão de Literatura}
\label{chap:fundamentacao_literatura}

Apresentam-se aqui os conceitos essenciais que embasam este trabalho: bancos de dados vetoriais para armazenamento e consulta de embeddings gerados por modelos de machine learning \cite{qwak2024integrating}; modelos de LLMs baseados em Transformer e seu uso no SEEU \cite{lewis2020rag,gao2023survey}; o método RAG, que une recuperação de documentos e geração de texto para reduzir alucinações \cite{edwards2024hybrid}; e, finalmente, o papel do PNUD e sua parceria com o CNJ em iniciativas de transformação digital e acesso à justiça \cite{undp2025sobre,undp2025pnudcnj}.

\section{Bancos de Dados Vetoriais}
\label{sec:bancos-vetoriais}

\subsection{Definição}
Bancos de dados vetoriais são sistemas especializados em armazenar, indexar e
consultar vetores em espaço multidimensional. Esses vetores — denominados
\emph{embeddings} — são gerados por modelos de \emph{machine learning} e
capturam características semânticas de dados não estruturados, como texto,
imagens, áudio e vídeo \cite{qwak2024integrating}.

\subsection{Importância}
A busca por similaridade em embeddings é fundamental para diversas
aplicações:
\begin{itemize}
  \item \textbf{Sistemas de recomendação} – identificação de itens similares às preferências do usuário;
  \item \textbf{Busca semântica} – consultas que interpretam o significado das palavras, não apenas correspondências exatas;
  \item \textbf{Reconhecimento de padrões} – detecção de faces, objetos ou outros padrões em grandes volumes de dados;
  \item \textbf{Pipelines de IA} – armazenamento eficiente de embeddings utilizados por modelos de \emph{deep learning}.
\end{itemize}
Tais bases oferecem consultas rápidas, baixa latência e alta escalabilidade —
qualidades essenciais em Natural Language Processing (\emph{NLP}), visão computacional e outros domínios que
envolvem grandes conjuntos de dados não estruturados
\cite{qwak2024integrating}.

\subsection{Análise comparativa de soluções}
\begin{description}
  \item[Elasticsearch:] plataforma distribuída e escalável com suporte a busca vetorial pelo \texttt{Elastic Vector Search}. Limitação: implementação vetorial ainda pouco madura em alta dimensionalidade.

  \item[OpenSearch:] \emph{fork} aberto do Elasticsearch com recursos vetoriais nativos e manutenção comunitária. Limitação: requer ajustes finos para consultas complexas.

  \item[PostgreSQL + \texttt{pgvector}:] integra dados relacionais e vetoriais em um mesmo SGBD. Limitação: desempenho inferior em buscas de larga escala.

  \item[Milvus:] banco vetorial especializado, otimizado para similaridade e escalável a bilhões de vetores. Limitação: maior complexidade de configuração e manutenção.

  \item[FAISS:] biblioteca de alto desempenho amplamente utilizada em pesquisa. Limitação: não é um SGBD completo, exigindo integração adicional.

  \item[Weaviate:] código aberto que combina buscas vetoriais e de grafo, permitindo consultas semânticas e relacionais. Limitação: requer \emph{tuning} avançado para desempenho ótimo.

  \item[Oracle Vector DB:] integração nativa ao ecossistema Oracle, com alta performance e segurança empresarial. Limitação: licenciamento oneroso e menor flexibilidade frente a soluções abertas \cite{oracle2025vector}.

  \item[IBM Vector DB:] forte integração com ferramentas de IA da IBM, oferecendo recursos robustos de análise vetorial. Limitação: custo elevado e configuração complexa \cite{ibm2025vector}.
  \end{description}

  \subsection{Conclusão}
    Nesta seção, foram apresentados os principais conceitos e aplicações dos bancos de dados vetoriais, bem como uma análise comparativa das soluções mais utilizadas no mercado. Verificou-se que a escolha da tecnologia adequada depende do equilíbrio entre desempenho, escalabilidade e requisitos organizacionais. Plataformas especializadas, como Milvus e FAISS, oferecem maior eficiência em buscas de similaridade, enquanto soluções integradas, como PostgreSQL+pgvector, favorecem a unificação de dados em um único SGBD. Por fim, espera-se que o contínuo aprimoramento nas técnicas de indexação e compressão de embeddings consolide ainda mais o papel dos bancos vetoriais em projetos de IA e processamento de dados semiestruturados.

%--------------------------------------------------------------------
% ------------------------------------------------------------
\section{Large Language Models (LLMs)}
\label{sec:llm}

\subsection{Introdução}
Os Modelos de LLMs, baseados na arquitetura
Transformer \cite{vaswani2017attention,naveeda2024comprehensive}, elevaram o
estado da arte em Processamento de Linguagem Natural (PLN), permitindo síntese,
tradução e interpretação semântica de documentos em larga escala. Essas redes neurais podem substituir buscas
puramente lexicais por consultas semânticas, aumentando a agilidade e a
precisão das respostas. A integração de LLMs a bancos de dados vetoriais
\cite{taipalus2024vector,qwak2024integrating} reforça essa capacidade,
fornecendo resultados contextualizados a partir de extensos acervos
documentais.

\subsection{Aspectos Técnicos}
\begin{enumerate}[label=\textbf{2.\arabic*}, leftmargin=*]
  \item \textbf{Pre-Training}\label{itm:pretraining}\\
        O modelo é submetido a um corpus genérico e volumoso para capturar
        padrões linguísticos amplos, formando uma base de conhecimento
        diversificada \cite{naveeda2024comprehensive}.
  
  \item \textbf{Fine-Tuning}\label{itm:finetuning}\\
        Realiza-se \emph{fine-tuning} com dados do domínio de conhecimento desejado.
        Estratégias de regularização (e.g., \textit{dropout}, \textit{early
        stopping}) evitam \textit{overfitting}. A eficácia é medida por
        precisão, \textit{recall} e F1-score
        \cite{yue2023disclawllm,lai2023lawm}.

% TODO: podemos colocar esse trecho em outro lugar?
  % \item \textbf{Desafios Técnicos}\label{itm:desafios}\\[-0.8em]
  %       \begin{itemize}
  %         \item Limpeza de dados — normalização de peças processuais;
  %         \item Seleção de hiperparâmetros — equilíbrio entre desempenho e custo;
  %         \item Escalabilidade — baixa latência com grandes volumes documentais
  %               \cite{edwards2024hybrid,pujiono2024implementing,aquino2024extracting}.
  %       \end{itemize}
  
  \item \textbf{Validação e Resultados Esperados}\label{itm:validacao}\\
        A avaliação inclui: (i) precisão na recuperação de informações;
        (ii) qualidade das respostas validadas por especialistas; e
        (iii) eficiência computacional face aos métodos atuais do SEEU.
        Resultados preliminares indicam ganhos expressivos de precisão e
        \textit{recall}.
\end{enumerate}

\subsection{Conclusão}
A combinação de \ref{itm:pretraining}–\ref{itm:validacao}, aliada a bancos
vetoriais, constitui abordagem inovadora para consultas jurídicas, reduzindo
prazos e aumentando a transparência do Judiciário
\cite{belarmino2025aplicacao,divald2021eformalization}. Estudos futuros podem
estender a metodologia a outros ramos do Direito, consolidando a transformação
digital no setor público.

%--------------------------------------------------------------------
\section{Retrieval-Augmented Generation (RAG)}
\label{sec:rag}

\subsection{Introdução}
O RAG associa a competência de LLMs em gerar texto à recuperação automática de
documentos, reduzindo \textit{alucinações} ao fundamentar as respostas em
evidências externas verificáveis
\cite{lewis2020rag,gao2023survey,edwards2024hybrid,pujiono2024implementing}.
LLMs armazenam conhecimento nos parâmetros (\emph{memória paramétrica}); já o
RAG adiciona uma \emph{memória não paramétrica} consultável em tempo real,
essencial em cenários como o SEEU, cujo acervo documental é volumoso e
dinâmico.

\subsection{Fundamentos}
\textbf{Data retrieval.} Consultas e documentos são convertidos em
\emph{embeddings}; métodos densos, como o \textit{Dense Passage Retrieval}
(DPR), aproximam vetores por similaridade de cosseno ou distância euclidiana,
retornando um subconjunto $k$-relevante
\cite{lewis2020rag,taipalus2024vector,mageirakos2025cracking}.\\
\textbf{Content generation.} Um modelo \emph{encoder--decoder} (ex.: BART ou
T5) concatena os trechos recuperados ao \textit{prompt} e gera a resposta. O
treinamento conjunto (Sec.~\ref{sec:rag:pipeline}) ensina o \textit{retriever}
a apresentar evidências úteis ao gerador
\cite{aquino2024extracting,belarmino2025aplicacao}.

\subsection{Pipeline}
\label{sec:rag:pipeline}
\begin{enumerate}[label=\arabic*.]
  \item \textbf{Ingestion} – extração de fontes estruturadas (bases SQL,Comprehensive Knowledge Archive Network (CKAN))
        e não estruturadas (PDF, HTML); limpeza, segmentação em parágrafos e
        criação de embeddings com modelos como \textit{all-MiniLM}.
        Objetos $\langle\text{ID},\,\text{embedding},\,\text{metadata}\rangle$
        são indexados em repositórios vetoriais (FAISS, Pinecone)
        \cite{qwak2024integrating,taipalus2024vector}.
  \item \textbf{Retrieval} – a consulta é vetorizada e comparada com o índice;
        top-$k$ documentos são ranqueados. Estratégias \emph{re-rank} com
        \textit{cross-encoders} ou fusão heurística (ex.: \textit{Reciprocal
        Rank Fusion}) aumentam precisão \cite{edwards2024hybrid}.
  \item \textbf{Treinamento conjunto} – ajuste \textit{end-to-end} de
        \textit{retriever} e \textit{generator} via
        \textit{maximum-likelihood} ou \textit{policy-gradient}, fazendo o
        \textit{retriever} maximizar a probabilidade da resposta correta
        \cite{zhang2025fine}.
\end{enumerate}

\subsection{Variantes}
\begin{itemize}
  \item \textbf{RAG-Sequence} – para cada um dos $k$ documentos recuperados, o modelo gera uma resposta completa de forma independente. Em seguida, calcula-se a probabilidade de cada resposta e faz a marginalização, ou seja, a combinação ponderada dessas probabilidades para produzir a resposta final. Esse método garante que cada documento tenha igual oportunidade de influenciar a resposta global, sendo indicado quando se deseja explorar várias interpretações completas antes da decisão final \cite{lewis2020rag,edwards2024hybrid}.
  \item \textbf{RAG-Token} – em vez de gerar respostas inteiras por documento, o modelo reavalia a distribuição de probabilidade a cada novo token, permitindo que diferentes documentos contribuam de forma pontual ao longo da geração. Isso amplia a cobertura informativa e mistura evidências de várias fontes, mas requer mecanismos adicionais de coerência para evitar que o texto final fique fragmentado ou inconsistente \cite{zhang2025fine}.
\end{itemize}

\subsection{Desafios e limitações}
\begin{itemize}
  \item \textbf{Latência} – cada consulta envolve busca vetorial $+$ geração,
        podendo ultrapassar limites de tempo real
        \cite{scalable2025overload}.
  \item \textbf{Atualização em tempo real} – garantir que o índice reflita
        alterações frequentes do corpus demanda pipelines de reingestão
        contínua \cite{taipalus2024vector}.
  \item \textbf{Qualidade da recuperação} – ruído ou pouca cobertura no índice
        reduz acurácia; técnicas de \textit{negative-sampling} e \textit{hard
        negatives} no treinamento mitigam o problema
        \cite{gao2023survey,salemi2024hallucination}.
  \item \textbf{Coerência textual} – fusão de múltiplas fontes pode gerar
        redundância ou mudança de estilo; pós-edição automática e
        penalidades de repetição auxiliam \cite{zhang2025fine}.
\end{itemize}

\subsection{Conclusão}
Esta seção apresentou o método RAG, que combina modelos de linguagem de grande porte com recuperação de documentos para reduzir alucinações e fundamentar respostas em evidências verificáveis. Descreveu-se o pipeline de ingestão, recuperação e treinamento conjunto, bem como variantes como RAG-Sequence e RAG-Token. Foram discutidos desafios relativos à latência, atualização em tempo real, qualidade da recuperação e coerência textual. Conclui-se que, apesar das limitações, o RAG representa avanço significativo para aplicações que exigem precisão e atualização dinâmica, sendo promissor para sistemas que trabalham com grandes acervos documentais, como o SEEU.

% ------------------------------------------------------------
\section{Programa das Nações Unidas para o Desenvolvimento (PNUD)}
\label{sec:pnud}

O PNUD é a agência da Organização das Nações Unidas (ONU) responsável por promover o desenvolvimento
humano sustentável e erradicar a pobreza em mais de 170 países e territórios
\cite{undp2025sobre,undp2025onu}. Sediado em Nova York, o PNUD oferece suporte
técnico e financeiro a políticas públicas voltadas às populações mais
vulneráveis.

\subsection{Objetivos e mandato}
O mandato do PNUD abrange quatro eixos centrais:
\begin{itemize}
  \item \textbf{Erradicação da pobreza} – programas para reduzir a pobreza
  extrema e melhorar as condições de vida;
  \item \textbf{Desigualdade e inclusão social} – políticas que promovem
  igualdade de oportunidades;
  \item \textbf{Desenvolvimento sustentável} – iniciativas que conciliam o uso
  de recursos naturais e a proteção ambiental;
  \item \textbf{Governança democrática} – fortalecimento institucional,
  transparência e participação cidadã.
\end{itemize}
Tais ações alinham-se à Agenda~2030 e aos Objetivos de Desenvolvimento
Sustentável (ODS), sobretudo o ODS~1 (pobreza) e o ODS~10 (redução das
desigualdades) \cite{wikipedia2025pnud}.

\subsection{Estrutura e funcionamento}
Financiado por contribuições voluntárias de Estados-membros, setor privado e
ONGs, o PNUD é chefiado por um administrador indicado pelo Secretário-Geral da
ONU e aprovado pela Assembleia Geral \cite{undp2025onu}. No Brasil, opera em
parceria com governos, sociedade civil e empresas, direcionando projetos que
fomentam o desenvolvimento sustentável e reduzem desigualdades
\cite{undp2025sobre}.

\subsection{Parceria CNJ–PNUD: direitos humanos e acesso à justiça}
\label{sec:cnj-pnud}

Em 2025, o CNJ e o PNUD firmaram acordo de
cooperação para fortalecer o Poder Judiciário na promoção de direitos humanos,
sustentabilidade socioambiental e acesso à justiça por populações
vulnerabilizadas \cite{undp2025pnudcnj}. O projeto complementa iniciativas como:
\begin{itemize}
  \item \textbf{Programa Justiça 4.0} – transformação digital do Judiciário
        brasileiro, ampliando transparência e celeridade processual;
  \item \textbf{Fazendo Justiça} – melhorias nas políticas de privação de
        liberdade e reintegração social.
\end{itemize}

A juíza auxiliar Karen Luise destaca que a ação se alinha à Estratégia 2021-2026
do CNJ, priorizando igualdade e acesso jurisdicional. Para o PNUD, a parceria
reforça o ODS~16, que visa instituições eficazes e inclusivas
\cite{undp2025pnudcnj}. As atividades previstas contemplam:
\begin{enumerate}
  \item fortalecimento institucional e capacitação de magistrados;
  \item diagnósticos situacionais e desenvolvimento de metodologias inclusivas;
  \item projetos-piloto focados em crianças e adolescentes em abrigamento,
        mulheres, pessoas LGBTQIA$+$, povos indígenas, pessoas em situação de
        rua, idosos, pessoas com deficiência e grupos vulneráveis por fatores
        socioambientais ou raciais.
\end{enumerate}

\subsection*{Impacto esperado}
O fortalecimento do sistema judiciário — por meio da digitalização,
capacitação e práticas inovadoras — tende a ampliar o acesso efetivo à justiça e
a reduzir barreiras estruturais. A cooperação CNJ–PNUD, portanto, contribui para
o cumprimento dos compromissos internacionais do Brasil relacionados aos ODS,
promovendo uma sociedade mais justa e inclusiva
\cite{undp2025pnudcnj}.

\subsection{Conclusão}
O PNUD configura-se como agente estratégico na promoção do desenvolvimento humano sustentável e na erradicação da pobreza, atuando em consonância com a Agenda 2030 e os Objetivos de Desenvolvimento Sustentável. A parceria firmada em 2025 entre o CNJ e o PNUD reforça esse compromisso, ao incorporar iniciativas de transformação digital, capacitação institucional e inclusão social no âmbito do Poder Judiciário brasileiro. Espera-se que tais ações ampliem o acesso efetivo à justiça para grupos vulnerabilizados e fortaleçam a governança democrática, contribuindo para o cumprimento das metas internacionais do Brasil e para a construção de uma sociedade mais equânime e participativa.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CAPÍTULO 3 – TECNOLOGIAS (revisado)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Tecnologias}

\label{chap:tecnologias}
Este capítulo apresenta as tecnologias empregadas: linguagens e bibliotecas de Machine Learning(ML)/NLP, indexação vetorial, modelos de linguagem, orquestração de pipeline e infraestrutura de deployment.  
\section{Linguagem e Bibliotecas Principais}
\begin{itemize}[label=\textbullet]

\item \textbf{Python} – linguagem base, com extensas bibliotecas para NLP e ML  \cite{python2024reference};
\item \textbf{Pandas} – manipulação de dados tabulares e pré-processamento de textos  \cite{pandas2024};
\item \textbf{NumPy} – operações vetoriais e matrizes de alto desempenho \cite{numpy2025};
\item \textbf{Scikit-learn} - para algoritmos para classificação, regressão, clustering e pré-processamento \cite{scikit-learn};



\end{itemize}

\section{Indexação e Recuperação Vetorial}
\subsection{OpenSearch}
O OpenSearch é um mecanismo de busca distribuído e de código aberto, baseado no Apache Lucene, feito para oferecer alta disponibilidade e escalabilidade no gerenciamento de grandes volumes de dados. Foi desenvolvido inicialmente pela AWS como um fork do Elasticsearch e atualmente mantido pela OpenSearch Software Foundation, ele tem suporte nativo a buscas vetoriais (k-NN) via plugins, essenciais para aplicações modernas como RAG e busca semântica. 
A versão 3.0 (2025) introduziu otimizações como aceleração por GPU para indexação mais rápida e múltiplas linguagens de consulta. Ele oferece recursos avançados como pipelines de processamento de buscas e monitoramento em tempo real via Live Queries API. \cite{taipalus2024vector,opensearch2025}.

\subsection{FAISS}
É uma biblioteca de código aberto desenvolvida pelo Facebook AI Research (FAIR) para busca de similaridade vetorial de alta performance em larga escala. Especializada em operações eficientes com bilhões de vetores. Utiliza técnicas avançadas como quantização de produto (PQ) e índices invertidos (IVF) para acelerar consultas k-NN em até 5x comparado a soluções convencionais. Sua arquitetura é otimizada para paralelismo em CPU/GPU e permite aplicações em tempo real como sistemas de recomendação, clustering de embeddings e RAG. O FAISS não gerencia metadados ou persistência nativamente, exigindo integração com sistemas externos como SQL databases ou caches distribuídos para cenários completos de produção \cite{facebook2024faiss}.

\subsection{Milvus}
Milvus é um banco de dados vetorial de código aberto projetado para armazenamento massivo e busca de similaridade de alta performance em escala de bilhões a trilhões de vetores. Sua arquitetura modular e distribuída separa armazenamento, computação e coordenação e permite escalonamento horizontal independente para cenários de alta demanda, como RAG e sistemas de recomendação. Com suporte a múltiplos algoritmos de indexação (ex: HNSW, IVF, DiskANN) e operações híbridas (combinação de busca vetorial com filtros de metadados), o Milvus otimiza consultas complexas em aplicações críticas de IA, como detecção de anomalias e busca multimodal\cite{milvus2025}.

\subsection{Weaviate}
Weaviate é um banco de dados vetorial orientado a objetos que combina armazenamento de vetores com dados estruturados, e mais usado em buscas híbridas (semântica + keyword via BM25) e consultas GraphQL. Projetado para prototipagem rápida e integração nativa com modelos de machine learning (ex: BERT, ResNet), suporta dados multimodais (texto, imagem) e oferece escalabilidade via sharding e réplicas. Sua arquitetura simplificada prioriza facilidade de uso em aplicações como classificação de conteúdo, motores de recomendação e RAG, especialmente em ambientes cloud-native \cite{weaviate2025}.


\section{Modelos de Linguagem}
\subsection{LLama}
LLaMA é um modelo de linguagem criado pela Meta que usa a tecnologia de transformadores para entender e gerar texto.Existem várias versões, com tamanhos que vão de 7 bilhões a 65 bilhões de parâmetros, o que indica a quantidade de informações que o modelo consegue processar.Ele foi treinado apenas com textos públicos, como sites, livros e artigos, mostrando que é possível obter bom desempenho sem usar bases de dados proprietárias.Versões menores, como o LLaMA-13B, alcançam resultados parecidos com modelos muito maiores, como o GPT-3, em tarefas de raciocínio e compreensão.Algumas inovações técnicas, como funções de ativação mais eficientes e ajustes no processo de aprendizado, fazem com que o LLaMA seja mais rápido e utilize menos recursos.Isso permite rodar o LLaMA em computadores comuns, sem a necessidade de grandes servidores, tornando-o acessível para pesquisas e aplicações em diversas áreas.
\cite{touvron2023llama}


\subsection{Hugging Face Transformers}
Hugging Face Transformers é uma biblioteca de código aberto que facilita o uso de modelos de linguagem baseados em arquiteturas Transformers. Ela oferece uma interface unificada para carregar e usar modelos pré-treinados como BERT, GPT, RoBERTa, T5 e muitos outros.Com poucos comandos em Python, é possível gerar texto, traduzir, classificar sentimentos ou responder perguntas, aproveitando o poder desses modelos avançados.A biblioteca é compatível com frameworks populares como PyTorch e TensorFlow, permitindo flexibilidade no desenvolvimento de soluções.Além disso, inclui utilitários para treinamento e afinamento (fine-tuning), o que ajuda pesquisadores e desenvolvedores a adaptar modelos a tarefas específicas.A documentação online é clara e enriquecida com exemplos práticos, tornando o Hugging Face Transformers acessível até para quem está começando.\cite{huggingface2024transformers}.

\section{Orquestração de Pipeline}
\subsection{LangChain}
LangChain é uma biblioteca de código aberto voltada para desenvolvimento de aplicações que utilizam grandes modelos de linguagem (LLMs).Ela fornece ferramentas para encadear chamadas a LLMs, gerenciar contextos e integrar diferentes componentes, como embeddings e bases de conhecimento.Com LangChain, é possível criar fluxos de trabalho sofisticados para agentes conversacionais, assistentes virtuais e sistemas de busca semântico.A biblioteca oferece suporte a diversos provedores de LLMs, como OpenAI e Hugging Face, facilitando a troca entre modelos.Além disso, inclui utilitários para armazenamento de estado, armazenamento vetorial e orquestração de múltiplos prompts.LangChain é ideal para quem deseja prototipar e escalar soluções baseadas em IA de forma modular e reutilizável.\cite{langchain2024}.

\subsection{Controle de Prompt e Fallback}
Para garantir resultados confiáveis, implementa-se um sistema de controle de prompt que valida a clareza e a completude da solicitação.Caso o LLM retorne uma resposta vaga ou incerta, a lógica de fallback direciona a requisição a uma base de dados oficial ou fonte confiável.Essa abordagem combina a flexibilidade dos modelos generativos com a precisão de dados verificados.O controle de prompt também permite reformular automaticamente perguntas problemáticas, melhorando a qualidade das interações.O mecanismo de fallback pode acessar APIs governamentais, documentos normativos ou bases de conhecimento internas.Dessa forma, o sistema assegura consistência e confiabilidade, minimizando riscos de informações errôneas.A estratégia possibilita uma experiência de usuário mais robusta, aliando criatividade dos LLMs ao respaldo de dados oficiais.

\section{Infraestrutura e Deployment}
\subsection{Docker}
Docker é uma plataforma que utiliza contêineres para empacotar, distribuir e executar aplicações de forma isolada.Cada contêiner contém tudo o que a aplicação precisa para rodar, incluindo bibliotecas, dependências e configurações.Isso garante consistência entre diferentes ambientes de desenvolvimento, teste e produção.Com Docker, é possível criar imagens imutáveis que facilitam o versionamento e a reprodução de ambientes.A ferramenta também simplifica a escalabilidade de serviços, permitindo o gerenciamento de múltiplos contêineres com orquestradores como Docker Compose ou Kubernetes.Além disso, a comunidade mantém um repositório público de imagens no Docker Hub, acelerando a adoção de soluções prontas.\cite{docker2024}


\subsection{Kubernetes}
Kubernetes é uma plataforma open source para orquestrar contêineres, permitindo implantar, escalar e operar aplicações em clusters de servidores.Ela agrupa contêineres em unidades chamadas pods, facilitando o gerenciamento e a distribuição de cargas de trabalho.O sistema inclui componentes como o kubelet, API Server, Scheduler e Controller Manager, que zelam pelo estado desejado do cluster.Com Kubernetes, é possível escalar serviços automaticamente com base em métricas de uso e garantir alta disponibilidade.A abstração de serviços e deployment controllers simplifica atualizações contínuas e rolling updates sem tempo de inatividade.Além disso, oferece mecanismos de descoberta de serviço, balanceamento de carga e armazenamento persistente para aplicações em contêiner.\cite{kubernetes2025overview}

\subsection{API REST com Flask}
Flask é um microframework em Python conhecido pela leveza e pela simplicidade de configuração, ideal para iniciar rapidamente o desenvolvimento de APIs REST.Ele utiliza o padrão WSGI e oferece flexibilidade para definir rotas e endereçar requisições HTTP sem impor estruturas rígidas.Com poucos comandos, é possível criar endpoints que recebem e retornam dados JSON, facilitando a comunicação com chatbots e outras aplicações externas.A extensibilidade do Flask permite integrar bibliotecas para autenticação, validação de dados e documentação automatizada (por exemplo, Swagger).Além disso, sua comunidade madura garante atualizações constantes e grande quantidade de tutoriais e exemplos práticos.O Flask também suporta a criação de blueprints, possibilitando modularizar a aplicação quando o projeto cresce em complexidade.Por essas razões, Flask é frequentemente escolhido para desenvolver serviços REST de forma ágil e confiável\cite{flask2024}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CAPÍTULO 4 – METODOLOGIA (revisado)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Metodologia}
\label{chap:metodologia}

\section{Arquitetura da Pipeline}
A pipeline combina quatro estágios principais: ingestão, pré‐processamento, indexação e geração. Cada estágio é containerizado e orquestrado via Kubernetes (Figura~\ref{fig:arquitetura_pipeline}).

\subsection{Coleta de Dados}
\begin{itemize}[label=\textbullet]
  \item Raspagem automática dos portais do CNJ usando Python e bibliotecas como \texttt{requests} e \texttt{BeautifulSoup};
  \item Agendamento via \emph{cron} ou Airflow para verificações mensal de novos documentos.
\end{itemize}

\subsection{Pré-processamento e Segmentação}
\begin{enumerate}[label=\arabic*.]
  \item Conversão de PDF para texto com \texttt{pdfplumber};
  \item Remoção de cabeçalhos, rodapés e caracteres especiais;
  \item Divisão em \emph{chunks} de 500--1000 tokens para otimizar a busca local.
\end{enumerate}

\subsection{Engenharia de Embeddings}
Escolha de modelo de embedding (BERTimbau ou OpenAI), parametrização de tamanho e normalização para garantir coerência semântica.

\subsection{Indexação e Armazenamento}
\begin{itemize}[label=\textbullet]
  \item Indexação dos embeddings em OpenSearch/FAISS com metadados (origem, data, posição);
  \item Definição de métricas de similaridade (cosine similarity) e thresholds de corte.
\end{itemize}

\subsection{Orquestração e Consulta}
Implementação em LangChain:
\begin{itemize}[label=\textbullet]
  \item Conversão da consulta em embedding;
  \item Recuperação dos top-k \emph{chunks};
  \item Geração da resposta pelo LLM, mesclando múltiplas fontes se necessário (\emph{RAG-Token}).
\end{itemize}

\subsection{Desenvolvimento do Chatbot e da API}
\begin{itemize}[label=\textbullet]
  \item Chatbot baseado em WebSocket para interação síncrona;
  \item Endpoints RESTful em Flask para consultas e feedback de usabilidade.
\end{itemize}

\subsection{Avaliação e Métricas}
\begin{itemize}[label=\textbullet]
  \item Precisão, recall e F1-Score em um conjunto de questões de benchmark (fase de testes);
  \item Ensaios de usabilidade qualitativos com operadores do Direito para avaliar intuitividade e confiança.
\end{itemize}

\subsection{MLOps e Monitoramento}
\begin{itemize}[label=\textbullet]
  \item Pipelines de CI/CD para build e deploy automáticos;
  \item Monitoramento de latência e acurácia em produção, com alertas para queda de desempenho;
  \item Feedback loop para re-treinamento periódico com dados reais de uso.
\end{itemize}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{04-figuras/arquitetura_pipeline.png}
  \caption{Arquitetura geral da pipeline RAG.}
  \label{fig:arquitetura_pipeline}
\end{figure}

% 4.2 Diagrama de Caso de Uso
\section{Diagrama de Caso de Uso}
\label{sec:diagrama-caso-uso}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{04-figuras/diagrama_em_branco.pdf}
  \caption{Diagrama de Caso de Uso da Pipeline RAG para consulta ao SEEU}
  \label{fig:diagrama-rag-seeu}
\end{figure}

\noindent
O diagrama da Figura~\ref{fig:diagrama-rag-seeu} apresenta os atores e casos de uso principais da pipeline RAG para o SEEU:  
\begin{itemize}
  \item \textbf{Operador Judiciário}: inicia a consulta via “Receber Pergunta”.  
  \item \textbf{Administrador}: configura parâmetros da pipeline em “Configurar Pipeline”.  
  \item \textbf{Sistema}: atua como ator externo encarregado de disparar e orquestrar os processos de “Coletar Documentos Públicos”, “Pré-processar e segmentar texto”, “Gerar Embeddings e atualizar índice” e “Realizar Consulta Semântica (RAG)”.  
  \item \textbf{Include} (setas obrigatórias): indicam os casos de uso que são sempre invocados no fluxo principal.  
  \item \textbf{Extend} (setas opcionais): representam funcionalidades adicionais, como a “Avaliar Métricas e Monitorar Desempenho”, que estende o caso de uso “Exibir Resposta via chatbot/API REST”.  
\end{itemize}  


\subsection{Especificação de Casos de Uso}
\label{sec:especificacao-casos-uso}
No item anterior, foi apresentado o Diagrama de Caso de Uso do sistema. 
A seguir serão detalhados cada um dos casos de uso, explicando as suas interações 
com os atores:

%----------------------------------------------------------------------
% UC-01 – Enviar Pergunta
%----------------------------------------------------------------------
\subsubsection{UC-01 – Enviar Pergunta}

\noindent
O \textit{Operador Judiciário} envia sua dúvida à interface de chatbot
para iniciar a consulta semântica.

\begin{table}[H]
\centering
\caption{Especificação do Caso de Uso UC-01 – Enviar Pergunta}
\label{tab:uc01}
\begin{tabular}{|p{4cm}|p{11cm}|}
\hline
\textbf{Nome do caso de uso} & UC-01 – Enviar Pergunta \\ \hline
\textbf{Ator Principal}      & Operador Judiciário \\ \hline
\textbf{Resumo}              & Registrar e validar a pergunta do usuário. \\ \hline
\textbf{Pré-Condições}       & Interface de chatbot online. \\ \hline
\textbf{Pós-Condições}       & Pergunta armazenada e evento emitido para \emph{UC-07}. \\ \hline
\multicolumn{2}{|l|}{\textbf{Fluxo Principal}} \\ \hline
\multicolumn{2}{|p{15cm}|}{%
\begin{enumerate}[label=\arabic*.,leftmargin=*]
  \item Operador acessa a interface.
  \item Digita a pergunta e clica em \textit{Enviar}.
  \item Sistema valida formato/tamanho.
  \item Sistema grava a pergunta e confirma recebimento.
\end{enumerate}} \\ \hline
\multicolumn{2}{|l|}{\textbf{Fluxo Alternativo}} \\ \hline
\multicolumn{2}{|p{15cm}|}{%
\begin{enumerate}[label=\arabic*a.,leftmargin=*]
  \item[3a.] Pergunta inválida $\rightarrow$ sistema exibe erro e retorna ao passo 2.
\end{enumerate}} \\ \hline
\end{tabular}
\end{table}


% UC-02
\subsubsection{UC-02 – Configurar Pipeline}
\noindent
O \textit{Administrador} define parâmetros e módulos da pipeline RAG.

\begin{table}[H]
\centering
\caption{Especificação do Caso de Uso UC-02 – Configurar Pipeline}
\label{tab:uc02}
\begin{tabular}{|p{4cm}|p{11cm}|}
\hline
\textbf{Nome do caso de uso}     & UC-02 – Configurar Pipeline \\ \hline
\textbf{Ator Principal}          & Administrador \\ \hline
\textbf{Resumo}                  & Ajustar fontes de coleta, periodicidade e parâmetros de indexação. \\ \hline
\textbf{Pré-Condições}           & Credenciais válidas de administrador. \\ \hline
\textbf{Pós-Condições}           & Parâmetros persistidos e \emph{UC-03 – Atualizar Base de Conhecimento} apto a ser agendado. \\ \hline
\multicolumn{2}{|l|}{\textbf{Fluxo Principal}} \\ \hline
\multicolumn{2}{|p{15cm}|}{%
  \begin{enumerate}[leftmargin=*]
    \item Administrador acessa o painel de configuração.
    \item Define fontes, cronograma e opções de pré-processamento e indexação.
    \item Salva alterações; sistema valida e aplica as configurações.
  \end{enumerate}} \\ \hline
\multicolumn{2}{|l|}{\textbf{Fluxo Alternativo}} \\ \hline
\multicolumn{2}{|p{15cm}|}{%
  \begin{enumerate}[label=\arabic*a.,leftmargin=*]
    \item[2a.] Valor inválido $\to$ sistema rejeita, exibe mensagem e retorna ao passo 2.
  \end{enumerate}} \\ \hline
\end{tabular}
\end{table}

% UC-03
\subsubsection{UC-03 – Atualizar Base de Conhecimento}
\noindent
Caso de uso de alto nível que executa todo o \textit{ETL} para manter o
índice vetorial sempre atualizado.

\begin{table}[H]
\centering
\caption{Especificação do Caso de Uso UC-03 – Atualizar Base de Conhecimento}
\label{tab:uc03}
\begin{tabular}{|p{4cm}|p{11cm}|}
\hline
\textbf{Nome do caso de uso}     & UC-03 – Atualizar Base de Conhecimento \\ \hline
\textbf{Ator Principal}          & Administrador \\ \hline
\textbf{Relações}                & {\small «include» UC-04, UC-05 e UC-06} \\ \hline
\textbf{Resumo}                  & Orquestrar a coleta, pré-processamento e (re)indexação dos dados. \\ \hline
\textbf{Pré-Condições}           & Parâmetros de pipeline configurados (UC-02). \\ \hline
\textbf{Pós-Condições}           & Índice vetorial contém todo o conteúdo recém-coletado. \\ \hline
\multicolumn{2}{|l|}{\textbf{Fluxo Principal}} \\ \hline
\multicolumn{2}{|p{15cm}|}{%
  \begin{enumerate}[leftmargin=*]
    \item Administrador dispara atualização ou CronJob executa no agendamento.
    \item Sistema aciona \emph{UC-04 – Coletar Documentos Públicos}.
    \item Sistema aciona \emph{UC-05 – Pré-processar e Segmentar Texto}.
    \item Sistema aciona \emph{UC-06 – Gerar Embeddings e Atualizar Índice}.
    \item Resultado agregado é registrado e notificado ao Administrador.
  \end{enumerate}} \\ \hline
\end{tabular}
\end{table}

%----------------------------------------------------------------------
% UC-04 – Coletar Documentos Públicos
%----------------------------------------------------------------------
\subsubsection{UC-04 – Coletar Documentos Públicos}

\noindent
Extrai automaticamente documentos dos portais SEEU/CNJ.

\begin{table}[H]
\centering
\caption{Especificação do Caso de Uso UC-04 – Coletar Documentos Públicos}
\label{tab:uc04}
\begin{tabular}{|p{4cm}|p{11cm}|}
\hline
\textbf{Nome}        & UC-04 – Coletar Documentos Públicos \\ \hline
\textbf{Ator Principal} & Sistema (Crawler) \\ \hline
\textbf{Resumo}      & Baixar PDFs/HTML/JSON dos portais e armazenar cópias brutas. \\ \hline
\textbf{Pré-Condições} & Fontes externas acessíveis. \\ \hline
\textbf{Pós-Condições} & Documentos brutos salvos para pré-processamento. \\ \hline
\multicolumn{2}{|l|}{\textbf{Fluxo Principal}} \\ \hline
\multicolumn{2}{|p{15cm}|}{%
\begin{enumerate}[leftmargin=*]
  \item Crawler inicia sessão nos portais.
  \item Pesquisa conteúdos novos.
  \item Faz download dos arquivos.
  \item Verifica integridade e armazena em repositório bruto.
\end{enumerate}} \\ \hline
\multicolumn{2}{|l|}{\textbf{Fluxo Alternativo}} \\ \hline
\multicolumn{2}{|p{15cm}|}{%
\begin{enumerate}[label=\arabic*a.,leftmargin=*]
  \item Falha de rede $\rightarrow$ reprograma tentativa e registra log.
  \item Documento corrompido $\rightarrow$ descarta e alerta administrador.
\end{enumerate}} \\ \hline
\end{tabular}
\end{table}

%----------------------------------------------------------------------
% UC-05 – Pré-processar e Segmentar Texto
%----------------------------------------------------------------------
\subsubsection{UC-05 – Pré-processar e Segmentar Texto}

\noindent
Converte PDFs em texto limpo e divide em \textit{chunks}.

\begin{table}[H]
\centering
\caption{Especificação do Caso de Uso UC-05 – Pré-processar e Segmentar Texto}
\label{tab:uc05}
\begin{tabular}{|p{4cm}|p{11cm}|}
\hline
\textbf{Nome}        & UC-05 – Pré-processar e Segmentar Texto \\ \hline
\textbf{Ator Principal} & Sistema (Pre-Processor) \\ \hline
\textbf{Resumo}      & Extrair texto, limpar ruídos e segmentar em \textit{chunks}. \\ \hline
\textbf{Pré-Condições} & Documentos brutos disponíveis. \\ \hline
\textbf{Pós-Condições} & \textit{Chunks} prontos para vetorização. \\ \hline
\multicolumn{2}{|l|}{\textbf{Fluxo Principal}} \\ \hline
\multicolumn{2}{|p{15cm}|}{%
\begin{enumerate}[leftmargin=*]
  \item Extrai texto de cada PDF/HTML.
  \item Remove cabeçalhos, rodapés e formatação.
  \item Separa texto em \textit{chunks} de 500–1000 tokens.
  \item Armazena \textit{chunks} para o Embedding Service.
\end{enumerate}} \\ \hline
\multicolumn{2}{|l|}{\textbf{Fluxo Alternativo}} \\ \hline
\multicolumn{2}{|p{15cm}|}{%
\begin{enumerate}[label=\arabic*a.,leftmargin=*]
  \item Falha na extração $\rightarrow$ registra alerta; prossegue com próximos arquivos.
\end{enumerate}} \\ \hline
\end{tabular}
\end{table}

%----------------------------------------------------------------------
% UC-06 – Gerar Embeddings e Atualizar Índice
%----------------------------------------------------------------------
\subsubsection{UC-06 – Gerar Embeddings e Atualizar Índice}

\noindent
Converte \textit{chunks} em vetores e atualiza o índice vetorial.

\begin{table}[H]
\centering
\caption{Especificação do Caso de Uso UC-06 – Gerar Embeddings e Atualizar Índice}
\label{tab:uc06}
\begin{tabular}{|p{4cm}|p{11cm}|}
\hline
\textbf{Nome}        & UC-06 – Gerar Embeddings e Atualizar Índice \\ \hline
\textbf{Ator Principal} & Sistema (Embedding Service) \\ \hline
\textbf{Resumo}      & Gerar embeddings e fazer \emph{upsert} no Vector DB. \\ \hline
\textbf{Pré-Condições} & \textit{Chunks} pré-processados disponíveis. \\ \hline
\textbf{Pós-Condições} & Índice vetorial atualizado. \\ \hline
\multicolumn{2}{|l|}{\textbf{Fluxo Principal}} \\ \hline
\multicolumn{2}{|p{15cm}|}{%
\begin{enumerate}[leftmargin=*]
  \item Seleciona \textit{chunks} não indexados.
  \item Calcula embedding com modelo pré-treinado.
  \item Envia vetor + metadados ao Vector DB.
  \item Recebe confirmação (\textit{ACK}).
\end{enumerate}} \\ \hline
\multicolumn{2}{|l|}{\textbf{Fluxo Alternativo}} \\ \hline
\multicolumn{2}{|p{15cm}|}{%
\begin{enumerate}[label=\arabic*a.,leftmargin=*]
  \item Falha no modelo $\rightarrow$ reprocessa \textit{chunk}; registra erro.
\end{enumerate}} \\ \hline
\end{tabular}
\end{table}


%----------------------------------------------------------------------
% UC-07 – Realizar Consulta Semântica (RAG)
%----------------------------------------------------------------------
\subsubsection{UC-07 – Realizar Consulta Semântica (RAG)}

\noindent
Executa a recuperação de contexto e geração de resposta via LLM.

\begin{table}[H]
\centering
\caption{Especificação do Caso de Uso UC-07 – Realizar Consulta Semântica (RAG)}
\label{tab:uc07}
\begin{tabular}{|p{4cm}|p{11cm}|}
\hline
\textbf{Nome}        & UC-07 – Realizar Consulta Semântica (RAG) \\ \hline
\textbf{Ator Principal} & Sistema (RAG Engine) \\ \hline
\textbf{Resumo}      & Vetorizar a pergunta, recuperar top-\emph{k} documentos e gerar resposta. \\ \hline
\textbf{Pré-Condições} & Pergunta armazenada (UC-01) e índice vetorial online. \\ \hline
\textbf{Pós-Condições} & Resposta gerada e log gravado. \\ \hline
\multicolumn{2}{|l|}{\textbf{Fluxo Principal}} \\ \hline
\multicolumn{2}{|p{15cm}|}{%
\begin{enumerate}[leftmargin=*]
  \item Converte pergunta em vetor.
  \item Recupera top-\emph{k} \textit{chunks} relevantes.
  \item Cria \emph{prompt} com contexto + pergunta.
  \item Chama LLM e obtém resposta.
  \item Formata resposta e grava log.
\end{enumerate}} \\ \hline
\multicolumn{2}{|l|}{\textbf{Fluxo Alternativo}} \\ \hline
\multicolumn{2}{|p{15cm}|}{%
\begin{enumerate}[label=\arabic*a.,leftmargin=*]
  \item Índice indisponível $\rightarrow$ retorna erro para UC-08.
\end{enumerate}} \\ \hline
\end{tabular}
\end{table}

%----------------------------------------------------------------------
% UC-08 – Exibir Resposta ao Usuário
%----------------------------------------------------------------------
\subsubsection{UC-08 – Exibir Resposta ao Usuário}

\noindent
Entrega a resposta ao Operador e registra entrega.

\begin{table}[H]
\centering
\caption{Especificação do Caso de Uso UC-08 – Exibir Resposta ao Usuário}
\label{tab:uc08}
\begin{tabular}{|p{4cm}|p{11cm}|}
\hline
\textbf{Nome}        & UC-08 – Exibir Resposta ao Usuário \\ \hline
\textbf{Ator Principal} & Operador Judiciário \\ \hline
\textbf{Relações}    & {\small «extend» UC-09 – Avaliar Métricas} \\ \hline
\textbf{Resumo}      & Formatar e entregar a resposta via WebSocket ou REST. \\ \hline
\textbf{Pré-Condições} & Resposta gerada no UC-07. \\ \hline
\textbf{Pós-Condições} & Resposta entregue e log de entrega salvo. \\ \hline
\multicolumn{2}{|l|}{\textbf{Fluxo Principal}} \\ \hline
\multicolumn{2}{|p{15cm}|}{%
\begin{enumerate}[leftmargin=*]
  \item Sistema formata resposta (HTML/JSON).
  \item Envia pelo canal apropriado.
  \item Recebe \textit{ACK} e registra status.
\end{enumerate}} \\ \hline
\end{tabular}
\end{table}

%----------------------------------------------------------------------
% UC-09 – Avaliar Métricas e Monitorar Desempenho
%----------------------------------------------------------------------
\subsubsection{UC-09 – Avaliar Métricas e Monitorar Desempenho}

\noindent
Calcula métricas (precisão, recall, latência) e dispara alertas.

\begin{table}[H]
\centering
\caption{Especificação do Caso de Uso UC-09 – Avaliar Métricas e Monitorar Desempenho}
\label{tab:uc09}
\begin{tabular}{|p{4cm}|p{11cm}|}
\hline
\textbf{Nome}        & UC-09 – Avaliar Métricas e Monitorar Desempenho \\ \hline
\textbf{Ator Principal} & Sistema (Metrics Service) \\ \hline
\textbf{Ator Secundário} & Administrador \\ \hline
\textbf{Resumo}      & Agregar logs, calcular métricas e atualizar dashboards. \\ \hline
\textbf{Pré-Condições} & Logs de UC-07 e UC-08 disponíveis. \\ \hline
\textbf{Pós-Condições} & Painel atualizado; alertas enviados se limites excedidos. \\ \hline
\multicolumn{2}{|l|}{\textbf{Fluxo Principal}} \\ \hline
\multicolumn{2}{|p{15cm}|}{%
\begin{enumerate}[leftmargin=*]
  \item Coleta dados de log/telemetria.
  \item Calcula precisão, recall, F1 e latência.
  \item Atualiza Painel Grafana/Prometheus.
  \item Se métrica fora do SLA, dispara alerta ao Administrador.
\end{enumerate}} \\ \hline
\end{tabular}
\end{table}


\subsubsection{Descrição}

A tela inicial tem como objetivo permitir que o operador jurídico registre sua pergunta de forma direta e intuitiva. Este é o ponto de entrada para a consulta semântica baseada em documentos do SEEU. Ao submeter a pergunta, o sistema realiza o roteamento interno, executando as etapas de recuperação vetorial e geração de resposta fundamentada por meio do modelo de linguagem integrado.

A funcionalidade central desta interface é a captação da pergunta e o acionamento da pipeline de RAG. O sistema registra a consulta, armazena informações relevantes como o usuário, horário da solicitação e a resposta gerada, contribuindo também para fins de auditoria e análise posterior de métricas.

\subsubsection{Comandos da tela (botões) – Tela inicial}

\begin{table}[H]
  \centering
  \caption{Quadro – Comandos da tela (botões) – Tela inicial}
  \label{tab:cmd_tela_inicial}
  \begin{tabular}{|c|p{4cm}|p{8cm}|}
    \hline
    \textbf{Item} & \textbf{Comando} & \textbf{Ação} \\ \hline
    1 & Enviar & Submete a pergunta digitada, aciona o pipeline RAG e exibe a resposta na área de conversa. \\ \hline
  \end{tabular}
\end{table}


\subsubsection{Campos da tela – Tela inicial}


\begin{table}[H]
  \centering
  \caption{Quadro 1 – Campos da tela – Tela Inicial}
  \label{tab:campos_tela_inicial}
  % -------- redimensiona para caber em \textwidth ----------
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{|c|p{4cm}|c|c|c|c|c|c|c|}
    \hline
    \textbf{Item} & \textbf{Nome do Campo} & \textbf{Tipo} & \textbf{Tamanho} & \textbf{Máscara} & \textbf{Obrigatório} & \textbf{Valor Padrão} & \textbf{Editável} & \textbf{Visível} \\ \hline
    1 & Entrada de Pergunta & Alfanumérico & Não aplicável & Não & Sim & -- & Sim & Sim \\ \hline
  \end{tabular}}
  \vspace{0.2cm}
\end{table}

\subsection{Tela de Configuração}
\label{subsec:tela_configuracao}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{04-figuras/image.png}
  \caption{Tela De Configuração}
  \label{fig:tela-configuracao}
\end{figure}

\subsubsection{Descrição}

A tela de configuração é destinada ao uso exclusivo de administradores e tem como finalidade assegurar a governança da solução, permitindo o ajuste de parâmetros críticos da pipeline RAG. As opções disponíveis incluem a ativação de filtros contra \emph{prompt injection}, a configuração da temperatura do modelo de linguagem e a definição dos domínios autorizados para busca vetorial.

Essas opções garantem maior segurança e conformidade jurídica, evitando abusos no uso da inteligência artificial e restringindo a atuação da pipeline apenas a documentos e fontes permitidas. Além disso, a configuração da temperatura permite calibrar a criatividade das respostas da LLM, adequando a formalidade e precisão do modelo ao contexto jurídico.

\subsubsection{Dicionário de Dados da Tela de Configuração}

\begin{table}[H]
  \centering
  \caption{Dicionário de Dados da Tela de Configuração}
  \label{tab:dd_tela_configuracao}
  \begin{tabular}{|p{3cm}|p{4cm}|p{8cm}|}
    \hline
    \textbf{Campo} & \textbf{Tipo} & \textbf{Descrição} \\ \hline
    \texttt{promptFilter} & \texttt{Boolean} & Ativa/desativa mecanismos de defesa contra \emph{prompt injection}. \\ \hline
    \texttt{temperature} & \texttt{float} & Valor de temperatura da LLM (ex: 0.0 a 1.0), influenciando a criatividade das respostas. \\ \hline
    \texttt{allowedDomains} & \texttt{List<String>} & Lista de domínios autorizados para busca vetorial nos documentos do SEEU. \\ \hline
    \texttt{adminUserId} & \texttt{String} & Identificador do administrador responsável pela alteração da configuração. \\ \hline
  \end{tabular}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CAPÍTULO 5 – RESULTADOS ESPERADOS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Resultados Esperados}
\label{chap:resultados}

Este capítulo descreve, de forma detalhada, os resultados que se almeja alcançar com a execução do presente Trabalho de Conclusão de Curso (TCC). Tais resultados derivam dos objetivos estabelecidos no Capítulo~\ref{sec:objetivos} e da metodologia apresentada no Capítulo~\ref{chap:metodologia}, respeitando as normas da Associação Brasileira de Normas Técnicas -- ABNT (NBR 14724:2011) para trabalhos acadêmicos.

\section{Implementação de uma pipeline RAG funcional}
Espera-se disponibilizar uma pipeline de RAG plenamente operacional, capaz de integrar um modelo de LLM a um índice vetorial que contenha os documentos públicos do SEEU. A solução deverá oferecer uma interface conversacional (chatbot) em língua portuguesa, além de uma API REST para consumo externo, permitindo consultas em linguagem natural e retornando respostas fundamentadas nos documentos originais.

\section{Ganho de eficiência na recuperação de informações}
A implementação proposta deverá reduzir o tempo médio despendido pelos operadores do Direito para localizar e consolidar informações no SEEU, quando comparado ao processo manual atualmente utilizado. Tal meta será verificada por meio de ensaios controlados que mensurem o tempo de resposta antes e depois da adoção da ferramenta. Objetiva-se uma redução de pelo menos 80\% no tempo de busca.

\section{Melhoria da qualidade das respostas}
Pretende-se atingir índices de precisão iguais ou superiores a 0,85, \emph{recall} mínimo de 0,80 e \emph{F\textsubscript{1}-score} não inferior a 0,82 na recuperação dos trechos mais relevantes, conforme protocolo de avaliação descrito na Seção~6.1. Esses valores garantirão a confiabilidade do sistema e a relevância das informações apresentadas ao usuário.

\section{Redução de alucinações do modelo}
A integração entre o LLM e o mecanismo de recuperação semântica deverá limitar a incidência de respostas não fundamentadas (alucinações) a, no máximo, 5\% do total de interações. Essa meta será monitorada por meio de amostragem estatística das conversas, com posterior verificação manual do conteúdo gerado.

\section{Escalabilidade e portabilidade comprovadas}
A solução será entregue em contêineres Docker, orquestrados via Kubernetes, garantindo a portabilidade entre ambientes e a escalabilidade horizontal necessária para lidar com picos de demanda, sem que a latência média de recuperação ultrapasse 1 s para consultas padrão.

\section{Integração institucional e impacto social}
Almeja-se que o protótipo desenvolvido se alinhe às iniciativas de transformação digital do Conselho Nacional de Justiça (Programa Justiça 4.0) e aos Objetivos de Desenvolvimento Sustentável nº 16 da Organização das Nações Unidas, contribuindo para a transparência e o acesso à justiça de populações vulnerabilizadas.

\section{Base para melhoria contínua}
Será implementado um mecanismo de \emph{feedback loop} que registre as interações dos usuários e permita o re-treinamento periódico do modelo de linguagem, assegurando a evolução constante do sistema e a adaptação às mudanças normativas ou procedimentais.

\section{Documentação técnica completa}
Serão entregues: código-fonte comentado, arquivos \texttt{Dockerfile}, manual do desenvolvedor, manual do usuário final e documentação da API. Essa documentação facilitará a reprodutibilidade acadêmica e a eventual adoção da solução por outros órgãos do Judiciário.

\section{Mitigação de riscos operacionais}
O projeto contemplará um plano de mitigação de riscos que inclua atualização contínua de dependências \emph{open source}, testes automatizados de regressão e políticas de segurança da informação, garantindo a confiabilidade e a sustentabilidade da aplicação em produção.

A consecução dos resultados elencados neste capítulo demonstrará a viabilidade técnica e o impacto prático da aplicação de técnicas de RAG na execução penal brasileira, servindo de base para futuras pesquisas e para possíveis expansões em âmbito nacional.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CAPÍTULO 6 – CONCLUSÕES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusões}
\label{chap:conclusoes}

Este Trabalho de Conclusão de Curso demonstrou a viabilidade de empregar uma \textit{pipeline} de \textit{Retrieval-Augmented Generation} (RAG) para qualificar a busca de informações no Sistema Eletrônico de Execução Unificado (SEEU). A integração entre modelos de linguagem de grande porte, índice vetorial e mecanismos de coleta e pré-processamento automatizados evidenciou ganhos expressivos de eficiência de acesso a leis e doutrinas jurídicas, além de dados de suporte de uso do sistema SEEU, contribuindo para reduzir o tempo despendido pelos operadores do Direito e mitigar o risco de respostas não fundamentadas.

Do ponto de vista técnico, a solução proposta consolidou um fluxo completo — da ingestão de texto e dados à disponibilização de respostas em linguagem natural, via \textit{chatbot} e API REST, sustentado em contêineres Docker e orquestração Kubernetes. A adoção de salvaguardas contra \textit{prompt injection}, a configuração de temperatura do modelo e a limitação de domínios de busca reforçaram a segurança e a conformidade jurídica do sistema.

Sob a ótica da gestão pública, a aplicação da RAG no SEEU tende a reduzir gargalos de produtividade e a fortalecer a transparência processual, alinhando-se às diretrizes do Programa Justiça 4.0 e ao Objetivo de Desenvolvimento Sustentável 16 da ONU. Além disso, a arquitetura modular apresentada favorece a escalabilidade e a replicação em outros tribunais e esferas do Poder Judiciário.

\subsection*{Limitações e trabalhos futuros}

Embora os resultados obtidos sejam promissores, algumas limitações foram identificadas: a necessidade de infraestrutura computacional especializada para hospedar o modelo, a dependência da qualidade dos documentos fonte e os desafios de manter o índice vetorial atualizado em tempo real. Como continuidade deste estudo, sugerem-se:

\begin{itemize}
  \item Expansão do corpus documental para incluir legislações estaduais, jurisprudência e doutrina correlata;
  \item Aperfeiçoamento do \textit{fine-tuning} com dados jurídicos específicos, a fim de elevar a precisão semântica das respostas;
  \item Implementação de rotinas de \textit{MLOps} que automatizem o re-treinamento do modelo quando forem detectadas quedas de desempenho;
  \item Avaliação de métodos avançados de explicabilidade (\textit{XAI}) para aumentar a confiança dos usuários nas respostas geradas.
\end{itemize}

Conclui-se, portanto, que a arquitetura desenvolvida oferece uma base sólida para a modernização da execução penal, abrindo caminho para novas pesquisas e melhorias contínuas no uso de inteligência artificial aplicada ao Poder Judiciário.